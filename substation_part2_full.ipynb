{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 禁用 Innov2（防误用）===\n",
    "class _Innov2Disabled:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Innov2 已禁用，请仅使用 Baseline 或 Innovation1\")\n",
    "\n",
    "Innovation2FaultDetector = _Innov2Disabled\n",
    "FusionClassifier = _Innov2Disabled\n",
    "TransformerFusionClassifier = _Innov2Disabled\n",
    "print('[Info] Innov2 已禁用：Innovation2FaultDetector / FusionClassifier / TransformerFusionClassifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bf358",
   "metadata": {},
   "source": [
    "\n",
    "# 变电站故障检测\n",
    "- 支持 Baseline / 创新点1 / 创新点2（先以轻量模式稳定跑通）  \n",
    "- 端到端：数据加载 → 训练 → 推理 → 评估  \n",
    "- 已内置 **11 个正确标签**：\n",
    "`'bj_bpmh', 'bj_bpps', 'bj_wkps', 'bjdsye', 'jyz_pl', 'sly_dmyw', 'hxg_gjbs', 'hxq_gjtps', 'xmbhyc', 'yw_gkxfw', 'yw_nc'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dcc0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | Classes: 11\n"
     ]
    }
   ],
   "source": [
    "import os, math, time, json, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if CUDA else 'cpu')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if CUDA: torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SAFE_MAX_PIXELS = 384 * 384   # 安全调整size\n",
    "USE_FP16 = True               # 启用amp\n",
    "TIMESTEPS_SAFE = [200]        # 开启稳定single-timestep\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'bj_bpmh', 'bj_bpps', 'bj_wkps', 'bjdsyc', 'jyz_pl', 'sly_dmyw',\n",
    "    'hxq_gjbs', 'hxq_gjtps', 'xmbhyc', 'yw_gkxfw', 'yw_nc'\n",
    "]\n",
    "print('Device:', DEVICE, '| Classes:', len(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caff8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：将 4D 图像张量（B,C,H,W）安全缩放到不超过 SAFE_MAX_PIXELS 的像素上限，避免显存暴涨。\n",
    "\n",
    "def safe_resize_bchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    B,C,H,W = x.shape\n",
    "    if H*W <= SAFE_MAX_PIXELS:\n",
    "        return x\n",
    "    scale = (SAFE_MAX_PIXELS / float(H*W)) ** 0.5\n",
    "    newH, newW = max(64, int(H*scale)), max(64, int(W*scale))\n",
    "    return F.interpolate(x, size=(newH,newW), mode='bilinear', align_corners=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1e76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 已启用更强数据增强与标准化\n"
     ]
    }
   ],
   "source": [
    "#功能：解析单个 VOC 风格 XML，提取类别名\n",
    "\n",
    "def parse_xml(xml_path: str) -> Optional[str]:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text.strip()\n",
    "            return name\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "#功能：自定义数据集类，从指定目录加载图像-标签对\n",
    "class SubstationDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, annos_dir: str, transform=None,\n",
    "                 class_names: List[str] = None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.annos_dir = Path(annos_dir)\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names or list(CLASS_NAMES)\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
    "        self.data_pairs = []\n",
    "        exts = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
    "        annos = [p for p in self.annos_dir.iterdir() if p.suffix.lower()=='.xml']\n",
    "        for ap in annos:\n",
    "            label = parse_xml(str(ap))\n",
    "            if label is None: \n",
    "                continue\n",
    "            if label not in self.class_to_idx:\n",
    "                continue\n",
    "            img_stem = ap.stem\n",
    "            img_candidate = None\n",
    "            for ext in exts:\n",
    "                ip = self.images_dir / f\"{img_stem}{ext}\"\n",
    "                if ip.exists():\n",
    "                    img_candidate = ip; break\n",
    "            if img_candidate is None:\n",
    "                for p in self.images_dir.iterdir():\n",
    "                    if p.suffix.lower() in exts and p.stem == img_stem:\n",
    "                        img_candidate = p; break\n",
    "            if img_candidate is not None:\n",
    "                self.data_pairs.append( (str(img_candidate), self.class_to_idx[label]) )\n",
    "        if len(self.data_pairs)==0:\n",
    "            print(\"[WARN] No matched image-xml pairs found.\")\n",
    "\n",
    "    def __len__(self): return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip, y = self.data_pairs[idx]\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "#功能：返回训练/验证两套 torchvision.transforms 变换\n",
    "#训练：随机水平/垂直翻转、颜色抖动、缩放、ToTensor\n",
    "#验证：缩放、ToTensor\n",
    "'''def get_transforms(img_size=256):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    return train_tf, val_tf'''\n",
    "\n",
    "    # === 更强数据增强（减轻过拟合）===\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transforms(img_size=256):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(0.3,0.3,0.3,0.15)], p=0.8),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 1.5))], p=0.3),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.14), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "print('[Info] 已启用更强数据增强与标准化')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db652034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#新增：无LabelSmoothing + Mixup + TTA\n",
    "\n",
    "# === 掩膜视图：在已有 train_tf 基础上叠加随机块遮挡 + RandomErasing ===\n",
    "class MaskedViewTransform:\n",
    "    def __init__(self, base_tfm, use_block_mask=True, use_random_erasing=True, max_blocks=2, erasing_p=0.25):\n",
    "        self.base_tfm = base_tfm\n",
    "        self.use_block_mask = use_block_mask\n",
    "        self.use_random_erasing = use_random_erasing\n",
    "        self.max_blocks = max_blocks\n",
    "        from torchvision import transforms\n",
    "        self.random_erasing = transforms.RandomErasing(p=erasing_p, value=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _block_mask(img_tensor, max_blocks=2):\n",
    "        # img_tensor: Tensor [C, H, W]\n",
    "        import random\n",
    "        c, h, w = img_tensor.shape\n",
    "        blocks = random.randint(1, max_blocks)\n",
    "        for _ in range(blocks):\n",
    "            rh = int(h * random.uniform(0.2, 0.35))\n",
    "            rw = int(w * random.uniform(0.2, 0.35))\n",
    "            y0 = random.randint(0, max(0, h - rh))\n",
    "            x0 = random.randint(0, max(0, w - rw))\n",
    "            img_tensor[:, y0:y0+rh, x0:x0+rw] = 0.0\n",
    "        return img_tensor\n",
    "\n",
    "    def __call__(self, pil_img):\n",
    "        x = self.base_tfm(pil_img)\n",
    "        if self.use_block_mask:\n",
    "            x = self._block_mask(x, self.max_blocks)\n",
    "        if self.use_random_erasing:\n",
    "            x = self.random_erasing(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SubstationDatasetFromPairsMV(Dataset):\n",
    "    \"\"\"训练：返回双视图 (view1, view2, label)\"\"\"\n",
    "    def __init__(self, pairs, tfm_view1, tfm_view2):\n",
    "        self.pairs = pairs\n",
    "        self.tfm1 = tfm_view1\n",
    "        self.tfm2 = tfm_view2\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, y = self.pairs[i]\n",
    "        from PIL import Image\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        x1 = self.tfm1(img)\n",
    "        x2 = self.tfm2(img)\n",
    "        return x1, x2, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22235694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5dd927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：扩散过程类，包含 q_sample 和 p_sample 方法\n",
    "#定义离散扩散过程的时间表与前向/反向采样公式（DDPM 公式族）\n",
    "class DiffusionProcess(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.0)\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "# 功能：前向采样，生成带噪声的图像(前向加噪)\n",
    "# 输入：原始图像 x_start, 时间步 t, 可选噪声 noise\n",
    "# 输出：带噪声的图像 x_noisy\n",
    "# 实现：根据时间步 t 的 alpha 和 beta 计算加噪强度，生成带噪声的图像\n",
    "# 关键点：用 sqrt(ᾱ_t)、sqrt(1-ᾱ_t) 线性混合 x_start 与 noise\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        acp = self.alphas_cumprod.to(t.device)\n",
    "        sqrt_acp = torch.sqrt(acp.gather(0, t)).view(-1,1,1,1)\n",
    "        sqrt_om = torch.sqrt((1.0 - acp).gather(0, t)).view(-1,1,1,1)\n",
    "        return sqrt_acp * x_start + sqrt_om * noise\n",
    "\n",
    "# 功能：反向采样，从带噪声的图像恢复原始图像(反向去噪,调用 U-Net 预测噪声,支持类条件传递)\n",
    "# 输入：U-Net 模型, 带噪声的图像 x, 时间步 t, 时间步索引 t_index, 可选类条件标签 class_labels\n",
    "# 输出：恢复的原始图像\n",
    "# 实现：根据时间步 t 的 alpha 和 beta 计算去噪强度，结合 U-Net 预测的噪声，恢复原始图像\n",
    "# 关键点：用 sqrt(ᾱ_t)、sqrt(1-ᾱ_t) 线性混合 x 与 U-Net 预测的噪声(类条件会被透传到去噪器)\n",
    "    def p_sample(self, model, x, t, t_index, class_labels=None):\n",
    "        betas_t = self.betas.to(t.device).gather(0, t).view(-1,1,1,1)\n",
    "        acp_t = self.alphas_cumprod.to(t.device).gather(0, t)\n",
    "        alphas_t = self.alphas.to(t.device).gather(0, t)\n",
    "        sqrt_one_minus = torch.sqrt(1.0 - acp_t).view(-1,1,1,1)\n",
    "        sqrt_recip = torch.sqrt(1.0 / alphas_t).view(-1,1,1,1)\n",
    "        pred_noise = model(x, t, class_labels=class_labels)\n",
    "        model_mean = sqrt_recip * (x - betas_t * pred_noise / sqrt_one_minus)\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        post_var_t = self.posterior_variance.to(t.device).gather(0, t).view(-1,1,1,1)\n",
    "        return model_mean + torch.sqrt(post_var_t) * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a2e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：U-Net 的残差块，包含时间条件、类条件传递、短连接等.\n",
    "#输入：输入特征 x, 时间嵌入 t_emb, 可选类条件 cond\n",
    "#输出：输出特征 h,(与 out_ch 匹配的特征图)\n",
    "#实现：\n",
    "# 1) 两个 3x3 卷积 + 时间条件 + 类条件传递\n",
    "# 2) 短连接：如果输入输出通道数相同，则直接相加；否则用 1x1 卷积调整通道数\n",
    "# 3) 使用 SiLU 激活函数，可选 Dropout 正则化\n",
    "class ResidBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, cond_dim=None, p=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.short = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.use_film = cond_dim is not None\n",
    "        if self.use_film:\n",
    "            self.gamma = nn.Linear(cond_dim, out_ch)\n",
    "            self.beta  = nn.Linear(cond_dim, out_ch)\n",
    "\n",
    "    def forward(self, x, t_emb, cond=None):\n",
    "        h = self.conv1(x)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        if self.use_film:\n",
    "            gamma = self.gamma(cond)[:, :, None, None]\n",
    "            beta  = self.beta(cond)[:, :, None, None]\n",
    "            h = h * (1 + gamma) + beta\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.short(x)\n",
    "\n",
    "#功能：增强 U-Net 模型,扩散去噪器（U-Net），支持时间条件、类条件传递、短连接等.\n",
    "#输入：输入特征 x, 时间步 timestep, 可选类条件 class_labels\n",
    "#输出：预测的噪声(与输入图像同尺寸的 3 通道张量)\n",
    "#实现：\n",
    "# 1) 时间条件：将时间步转换为嵌入，与特征图拼接\n",
    "# 2) 类条件：如果提供，则转换为可学习的提示，与特征图拼接\n",
    "# 3) 残差块：包含两个 3x3 卷积 + 时间条件 + 类条件传递 + 短连接\n",
    "# 4) 短连接：如果输入输出通道数相同，则直接相加；否则用 1x1 卷积调整通道数\n",
    "# 5) 使用 SiLU 激活函数，可选 Dropout 正则化\n",
    "# 6) 使用 avg_pool2d 下采样，使用 F.interpolate 上采样\n",
    "# 7) 使用 torch.cat 拼接特征图，实现跳跃连接\n",
    "# 8) 使用 nn.Conv2d 输出预测的噪声\n",
    "class EnhancedUNet(nn.Module):\n",
    "    def __init__(self, num_classes: int, base_ch=64, use_film=True, use_attention=False, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        ch = int(base_ch * width_mult)\n",
    "        self.use_film = use_film\n",
    "        self.use_attention = use_attention\n",
    "        time_dim = ch * 4\n",
    "        cond_dim = ch * 2 if use_film else None\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, ch), nn.SiLU(),\n",
    "            nn.Linear(ch, time_dim)\n",
    "        )\n",
    "        self.learnable_prompts = nn.Embedding(num_classes, cond_dim if use_film else 1)\n",
    "\n",
    "        # encoder\n",
    "        self.enc1 = ResidBlock(3,      ch,   time_dim, cond_dim)\n",
    "        self.enc2 = ResidBlock(ch,     ch*2, time_dim, cond_dim)\n",
    "        self.enc3 = ResidBlock(ch*2,   ch*4, time_dim, cond_dim)\n",
    "        self.mid  = ResidBlock(ch*4,   ch*4, time_dim, cond_dim)\n",
    "\n",
    "        # decoder (concat skips)\n",
    "        self.dec3 = ResidBlock(ch*4 + ch*2, ch*2, time_dim, cond_dim)  # 6*ch -> 2*ch\n",
    "        self.dec2 = ResidBlock(ch*2 + ch,   ch,   time_dim, cond_dim)  # 3*ch -> 1*ch\n",
    "        self.dec1 = nn.Conv2d(ch, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, timestep, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            class_labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        t = timestep.view(-1, 1).float()\n",
    "        t_emb = self.time_mlp(t)\n",
    "        cond = self.learnable_prompts(class_labels) if self.use_film else None\n",
    "\n",
    "        # encoder\n",
    "        e1 = self.enc1(x,               t_emb, cond)        # [B, ch,   H,   W]\n",
    "        e2 = self.enc2(F.avg_pool2d(e1, 2), t_emb, cond)    # [B, 2ch,  H/2, W/2]\n",
    "        e3 = self.enc3(F.avg_pool2d(e2, 2), t_emb, cond)    # [B, 4ch,  H/4, W/4]\n",
    "        m  = self.mid(e3,               t_emb, cond)        # [B, 4ch,  H/4, W/4]\n",
    "\n",
    "        # decoder with concat skips\n",
    "        up_m = F.interpolate(m, scale_factor=2, mode='nearest')        # [B, 4ch, H/2, W/2]\n",
    "        #d3_in = torch.cat([up_m, e2], dim=1)                           # [B, 6ch, H/2, W/2]\n",
    "        d3 = self.dec3(torch.cat([up_m, e2], dim=1), t_emb, cond)                             # [B, 2ch, H/2, W/2]\n",
    "\n",
    "        up_d3 = F.interpolate(d3, scale_factor=2, mode='nearest')      # [B, 2ch, H, W]\n",
    "        #d2_in = torch.cat([up_d3, e1], dim=1)                          # [B, 3ch, H, W]\n",
    "        d2 = self.dec2(torch.cat([up_d3, e1], dim=1), t_emb, cond)                              # [B, ch,  H, W]\n",
    "\n",
    "        out = self.dec1(d2)                                            # [B, 3,   H, W]\n",
    "        return out\n",
    "  # predicted noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e8315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差提取与多尺度\n",
    "#功能：多尺度残差提取器，支持多尺度残差提取与扩散去噪.利用扩散反推把输入图像“还原”到接近正常，再与原图取绝对差，得到残差；可按多时间步提取。\n",
    "#输入：扩散去噪器, 时间步列表\n",
    "#输出：多尺度残差列表\n",
    "#实现：\n",
    "# 1) 初始化：保存扩散去噪器与时间步列表\n",
    "# 2) 前向：对每个时间步，调用扩散去噪器进行反向采样，得到残差\n",
    "# 3) 返回：所有时间步的残差列表\n",
    "# 4) 使用 torch.no_grad() 装饰器，避免梯度计算\n",
    "class MultiScaleResidualExtractor(nn.Module):\n",
    "    def __init__(self, diffusion_model: EnhancedUNet, timesteps: List[int]):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.timesteps = timesteps\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "\n",
    "#功能：提取单个时间步的残差,从给定时间步 t_value 反推到 0，得到还原图 x̂ 并计算 |x - x̂|\n",
    "#输入：输入图像 x, 标签 labels, 时间步 t_value, 步数 steps\n",
    "#输出：残差\n",
    "#关键点：每步都携带 class_labels（innov1 的类条件反推）\n",
    "#实现：\n",
    "# 1) 初始化：保存扩散去噪器与时间步列表\n",
    "# 2) 前向：对每个时间步，调用扩散去噪器进行反向采样，得到残差\n",
    "# 3) 返回：所有时间步的残差列表\n",
    "# 被谁调用：验证/推理，多尺度时会被多次调用后堆叠。\n",
    "    @torch.no_grad()\n",
    "    def extract_one_timestep(self, x, labels, t_value: int, steps: int = 8):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev, non_blocking=True)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_value // max(1, steps)))\n",
    "        for i in range(t_value, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        residual = (x - x_t).abs()\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c25d7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 已覆盖 ResidualClassifier：加入 Dropout 与 LabelSmoothing 支持（由 Trainer 使用）\n"
     ]
    }
   ],
   "source": [
    "#分类器与多尺度融合\n",
    "#功能：残差分类器，将多尺度残差堆叠后，用 ResNet50 提取特征，最后用全连接层分类.对单张残差图做分类。\n",
    "#输入：多尺度残差列表\n",
    "#输出：分类结果\n",
    "#关键点：ResNet50(IMAGENET1K_V2) 作为骨干，只替换最后全连接层输出通道数\n",
    "#实现：\n",
    "# 1) 初始化：加载 ResNet50 模型，替换最后一层全连接为 num_classes 输出\n",
    "# 2) 前向：对每个多尺度残差，用 ResNet50 提取特征，最后用全连接层分类\n",
    "# 3) 返回：分类结果\n",
    "# 被谁调用：验证/推理，多尺度时会被多次调用后堆叠。\n",
    "'''class ResidualClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        in_feat = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_feat, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)'''\n",
    "\n",
    "# === 覆盖 ResidualClassifier：更强正则（Dropout/LabelSmoothing 支持）===\n",
    "class ResidualClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, p_drop=0.4, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        in_feat = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_feat, num_classes)\n",
    "        )\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print('[Info] 已覆盖 ResidualClassifier：加入 Dropout 与 LabelSmoothing 支持（由 Trainer 使用）')\n",
    "\n",
    "\n",
    "\n",
    "#功能：多尺度融合分类器，将多尺度残差堆叠后，用 2 层 3x3 卷积 + 平均池化 + 全连接层分类.\n",
    "#输入：残差栈 (B,T,C,H,W)\n",
    "#关键点：先把 (T,C,H,W) 当“伪通道”或展平做轻量卷积/池化，再接全连接层。\n",
    "#被谁调用：Innov2（多时间步） \n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, num_timesteps: int = 1, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,8))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*8*8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x): # [B,T,C,H,W]\n",
    "        if x.dim()==4:\n",
    "            x = x.unsqueeze(1)\n",
    "        B,T,C,H,W = x.shape\n",
    "        x = x.mean(dim=1)  # avg over time\n",
    "        h = self.conv(x)\n",
    "        h = self.pool(h).view(B,-1)\n",
    "        return self.fc(h)\n",
    "\n",
    "# 功能：Transformer 融合分类器，将多尺度残差堆叠后（把残差栈先做网格池化到固定维度，投影为序列），用 Transformer 提取特征，最后用全连接层分类.\n",
    "#把 (B,T,C,H,W) 先做网格池化到固定维度，投影为序列，再用 TransformerEncoder 做时序/尺度建模。\n",
    "#残差栈 (B,T,C,H,W) → 网格池化 → 序列 (B,T,C*g*g) → TransformerEncoder → 平均池化 (B,D) → 全连接 (B,num_classes)\n",
    "#关键点：每一帧残差提取一个 token，Transformer 编码，做时序平均池化再分类。\n",
    "#被谁调用：Innov2（多时间步 + Transformer 融合）\n",
    "class TransformerFusionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11, num_timesteps=1, in_channels=3, grid=8,\n",
    "                 proj_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.grid = grid\n",
    "        self.proj = nn.Linear(in_channels * grid * grid, proj_dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=proj_dim, nhead=8, dim_feedforward=1024, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(proj_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):  # [B,T,C,H,W]\n",
    "        if x.dim()==4:\n",
    "            x = x.unsqueeze(1)\n",
    "        B,T,C,H,W = x.shape\n",
    "        feats = []\n",
    "        for i in range(T):\n",
    "            f = F.adaptive_avg_pool2d(x[:,i], (self.grid, self.grid))  # [B,C,g,g]\n",
    "            f = f.view(B, -1)                                          # [B, C*g*g]\n",
    "            f = self.proj(f)                                           # [B, D]\n",
    "            feats.append(f)\n",
    "        seq = torch.stack(feats, dim=1)                                # [B,T,D]\n",
    "        enc = self.encoder(seq).mean(dim=1)                            # [B,D]\n",
    "        return self.fc(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bc691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 端到端模型封装\n",
    "# 功能：封装了创新1/2的模型，包括扩散去噪器、多尺度残差提取器、残差分类器、Transformer 融合分类器。\n",
    "# 基线总成：DiffusionProcess + EnhancedUNet(use_film=False) + ResidualClassifier\n",
    "# 输入：图像 x, 标签 labels, 模式 mode\n",
    "# 输出：预测的噪声, 噪声, 时间步, 标签\n",
    "# 关键点：训练时残差支路不回传梯度（通过 no_grad 或单独函数），扩散器只用 MSE 学“正常先验”，分类器用 CE 学“看残差分类”\n",
    "\n",
    "\n",
    "class BaselineFaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11, img_size=256):\n",
    "        super().__init__()\n",
    "        self.diffusion = DiffusionProcess()\n",
    "        self.unet = EnhancedUNet(num_classes=num_classes, use_film=False, width_mult=0.75)\n",
    "        self.classifier = ResidualClassifier(num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = self.diffusion.q_sample(x, t, noise)\n",
    "            pred_noise = self.unet(x_noisy, t, class_labels=None)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                res = MultiScaleResidualExtractor(self.unet, TIMESTEPS_SAFE).extract_one_timestep(\n",
    "                    x, labels if labels is not None else torch.zeros(x.size(0), dtype=torch.long, device=x.device),\n",
    "                    t_value=200, steps=8\n",
    "                )\n",
    "                logits = self.classifier(res)\n",
    "                return res.unsqueeze(1), logits\n",
    "\n",
    "# 创新1总成：DiffusionProcess + EnhancedUNet(use_film=True) + ResidualClassifier\n",
    "# U-Net 开启 use_film=True，并在反推/训练都传入 class_labels\n",
    "# 关键点：FiLM 条件：nn.Embedding(num_classes, cond_dim) 生成类条件向量，注入各残差块；\n",
    "# 反推时也传 labels 给 p_sample，确保“按该类正常形态”还原；\n",
    "# 保持损失组合 MSE + 0.5×CE 与解耦训练不变。\n",
    "\n",
    "class Innovation1FaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "        self.diffusion_model = EnhancedUNet(num_classes=num_classes, use_film=True, width_mult=0.75)\n",
    "        self.classifier = ResidualClassifier(num_classes)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct_residual_fast(self, x, labels, t_start=200, steps=10):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_start // max(1, steps)))\n",
    "        for i in range(t_start, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        return (x - x_t).abs()\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if labels is None:\n",
    "            labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion_process.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = self.diffusion_process.q_sample(x, t, noise)\n",
    "            pred_noise = self.diffusion_model(x_noisy, t, class_labels=labels)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                res = self.reconstruct_residual_fast(x, labels, t_start=200, steps=8)\n",
    "                logits = self.classifier(res)\n",
    "                return res.unsqueeze(1), logits\n",
    "\n",
    "# 创新2总成：DiffusionProcess + EnhancedUNet(use_film=True) + MultiScaleResidualExtractor + FusionClassifier/TransformerFusionClassifier\n",
    "class Innovation2FaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11, use_transformer=True, timesteps=[200], lightweight=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.timesteps = timesteps\n",
    "        self.lightweight = lightweight\n",
    "\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "        self.diffusion_model = EnhancedUNet(num_classes=num_classes, use_film=True,\n",
    "                                            use_attention=(False if lightweight else True),\n",
    "                                            width_mult=(0.75 if lightweight else 1.0))\n",
    "        self.residual_extractor = MultiScaleResidualExtractor(self.diffusion_model, self.timesteps)\n",
    "        if lightweight or not use_transformer:\n",
    "            self.fusion_classifier = FusionClassifier(num_classes=num_classes, num_timesteps=len(self.timesteps))\n",
    "        else:\n",
    "            self.fusion_classifier = TransformerFusionClassifier(num_classes=num_classes, num_timesteps=len(self.timesteps))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct_residual_fast(self, x, labels, t_start=200, steps=8):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_start // max(1, steps)))\n",
    "        for i in range(t_start, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        return (x - x_t).abs()\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if labels is None:\n",
    "            labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion_process.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            masked_x = x\n",
    "            x_noisy = self.diffusion_process.q_sample(masked_x, t, noise)\n",
    "            pred_noise = self.diffusion_model(x_noisy, t, class_labels=labels)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            self.eval()\n",
    "            dev = next(self.parameters()).device\n",
    "            timesteps = self.timesteps if (self.lightweight is False) else TIMESTEPS_SAFE\n",
    "            with torch.inference_mode():\n",
    "                residual_list = []\n",
    "                for t in timesteps:\n",
    "                    res_t = self.reconstruct_residual_fast(x, labels, t_start=int(t), steps=8)  # [B,C,H,W]\n",
    "                    residual_list.append(res_t)\n",
    "                res_stack = torch.stack(residual_list, dim=1)  # [B,T,C,H,W]\n",
    "                logits = self.fusion_classifier(res_stack.to(next(self.fusion_classifier.parameters()).device))\n",
    "                return res_stack, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac0da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "AMP_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# 训练器\n",
    "# 功能：训练模型，包括前向、损失计算、反向传播、梯度裁剪、优化器更新等；封装优化器、损失、AMP、单轮训练与验证逻辑、保存最优模型。\n",
    "# 关键点：使用 GradScaler 处理混合精度训练，使用交叉熵损失计算分类误差，使用 MSE 损失计算扩散损失。\n",
    "# 优化器：AdamW；混合精度：GradScaler；\n",
    "# 损失：mse = MSE(pred_noise, noise)；ce = CE(logits, y)；总损失 loss = mse + 0.5*ce；\n",
    "# 保存：当 val_acc 创新高就 torch.save\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, lr=2e-4, weight_decay=5e-4):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #self.scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.scaler = GradScaler(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "#对每个 batch：采样 t 加噪 → U-Net 预测噪声 → MSE；无梯度重构残差 → 分类器 → CE；loss = MSE + 0.5×CE 反传更新；统计训练准确率。\n",
    "# 可选：用 train_eval_loader 在自然分布上再测一次 train_acc。\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            self.optim.zero_grad(set_to_none=True)\n",
    "            #with torch.cuda.amp.autocast(enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "            with autocast(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "                out = self.model(imgs, labels=ys, mode='train')\n",
    "                if isinstance(out, tuple):\n",
    "                    pred_noise, noise, t, _ = out\n",
    "                    diffusion_loss = self.mse(pred_noise, noise)\n",
    "                else:\n",
    "                    diffusion_loss = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "                        res = self.model.reconstruct_residual_fast(imgs, ys, t_start=200, steps=8)\n",
    "                    else:\n",
    "                        res = MultiScaleResidualExtractor(self.model.unet, TIMESTEPS_SAFE).extract_one_timestep(imgs, ys, t_value=200, steps=8)\n",
    "                if hasattr(self.model, 'fusion_classifier'):\n",
    "                    logits = self.model.fusion_classifier(res.unsqueeze(1).to(next(self.model.fusion_classifier.parameters()).device))\n",
    "                else:\n",
    "                    logits = self.model.classifier(res)\n",
    "                ce = self.ce(logits, ys)\n",
    "                loss = diffusion_loss + 0.5 * ce\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                total += ys.size(0)\n",
    "                loss_sum += loss.item() * ys.size(0)\n",
    "\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "# 验证器:验证评估。走推理路径（反推→残差→分类），统计 val_loss/val_ac\n",
    "# 关键点：不做反传；与训练的残差支路保持一致的生成逻辑；遇到更高 acc 即保存 best\n",
    "    @torch.no_grad()\n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            res_stack, logits = self.model(imgs, labels=ys, mode='eval')\n",
    "            ce = self.ce(logits, ys)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==ys).sum().item()\n",
    "            total += ys.size(0)\n",
    "            loss_sum += ce.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ddb1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights_ce: [0.504 0.577 0.835 0.613 0.999 0.565 0.399 4.314 1.15  0.576 0.468]\n",
      "train/val sizes: 5216 1304\n",
      "class counts (train): [616 538 372 507 311 550 778  72 270 539 663]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IMAGES_DIR = '/mnt/e/code/project/Dataset-total/images'   # 修改为你的路径\n",
    "ANNOTATIONS_DIR = '/mnt/e/code/project/Dataset-total/annotations/xmls'  # 修改为你的路径\n",
    "\n",
    "img_size = 256\n",
    "val_ratio = 0.2\n",
    "bs = 8\n",
    "workers = 4\n",
    "\n",
    "'''train_tf, val_tf = get_transforms(img_size)\n",
    "base = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=None, class_names=list(CLASS_NAMES))\n",
    "indices = np.arange(len(base))\n",
    "if len(indices)==0:\n",
    "    raise RuntimeError('No data found. Please check paths.')\n",
    "labels_np = np.array([ base.data_pairs[i][1] for i in indices ])\n",
    "\n",
    "train_idx, val_idx = train_test_split(indices, test_size=val_ratio, random_state=SEED, stratify=labels_np)\n",
    "\n",
    "train_ds = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=train_tf, class_names=list(CLASS_NAMES))\n",
    "val_ds   = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=val_tf,   class_names=list(CLASS_NAMES))\n",
    "train_ds = Subset(train_ds, train_idx.tolist())\n",
    "val_ds   = Subset(val_ds,   val_idx.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) 只构建一次数据集，拿到稳定的 pairs\n",
    "_base = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=None, class_names=list(CLASS_NAMES))\n",
    "pairs = _base.data_pairs  # [(img_path, class_idx), ...]\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    raise RuntimeError('No data found. Check IMAGES_DIR / ANNOTATIONS_DIR.')\n",
    "\n",
    "# 2) 按同一份 pairs 做可复现的划分\n",
    "indices = np.arange(len(pairs))\n",
    "labels_np = np.array([ y for _, y in pairs ])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=val_ratio, random_state=SEED, stratify=labels_np)\n",
    "\n",
    "# 3) 用“同一清单 + 不同 transform”构建 Dataset 封装器，避免重新枚举文件\n",
    "'''class SubstationDatasetFromPairs(Dataset):\n",
    "    def __init__(self, pairs, transform=None):\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, y = self.pairs[i]\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "train_tf, val_tf = get_transforms(img_size)\n",
    "train_ds = SubstationDatasetFromPairs([pairs[i] for i in train_idx], transform=train_tf)\n",
    "val_ds   = SubstationDatasetFromPairs([pairs[i] for i in val_idx],   transform=val_tf)'''\n",
    "\n",
    "# 新增：无LabelSmoothing + Mixup + TTA\n",
    "# 3) 用“同一清单 + 不同 transform”构建 Dataset 封装器（训练=双视图，验证=单视图）\n",
    "class SubstationDatasetFromPairs(Dataset):\n",
    "    def __init__(self, pairs, transform=None):\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, y = self.pairs[i]\n",
    "        from PIL import Image\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "train_tf, val_tf = get_transforms(img_size)\n",
    "\n",
    "# === 新增：掩膜视图（在 train_tf 基础上叠加遮挡/擦除）===\n",
    "masked_train_tf = MaskedViewTransform(\n",
    "    base_tfm=train_tf,\n",
    "    use_block_mask=True,\n",
    "    use_random_erasing=True,\n",
    "    max_blocks=2,\n",
    "    erasing_p=0.25\n",
    ")\n",
    "\n",
    "# 训练集换成双视图（view1=原训练增强；view2=掩膜训练增强）\n",
    "train_ds = SubstationDatasetFromPairsMV([pairs[i] for i in train_idx], tfm_view1=train_tf, tfm_view2=masked_train_tf)\n",
    "# 验证集保持单视图\n",
    "val_ds   = SubstationDatasetFromPairs([pairs[i] for i in val_idx],   transform=val_tf)\n",
    "\n",
    "\n",
    "# 4) loader：训练用加权采样，评估用自然分布\n",
    "use_weighted_sampler = True\n",
    "\n",
    "if use_weighted_sampler:\n",
    "    train_labels = [ y for _, y in [pairs[i] for i in train_idx] ]\n",
    "    counts = np.bincount(train_labels, minlength=len(CLASS_NAMES))\n",
    "    #class_weights = 1.0 / np.clip(counts, a_min=1, a_max=None)\n",
    "    inv = 1.0 / np.clip(counts, a_min=1, a_max=None)\n",
    "    class_weights_ce = (inv / inv.sum()) * len(inv)          # 归一到均值≈1\n",
    "    CLASS_WEIGHTS_TENSOR = torch.tensor(class_weights_ce, dtype=torch.float32, device=DEVICE)\n",
    "    print('class_weights_ce:', np.round(class_weights_ce, 3))\n",
    "    \n",
    "    sample_weights = [ class_weights_ce[y] for y in train_labels ]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, sampler=sampler, num_workers=workers, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "# 验证集始终用自然分布 + 顺序采样\n",
    "val_loader = DataLoader(val_ds,   batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "# 另外做一个“训练集评估 loader”（自然分布），用于客观的 train_acc\n",
    "train_eval_loader = DataLoader(train_ds, batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "\n",
    "print('train/val sizes:', len(train_ds), len(val_ds))\n",
    "if use_weighted_sampler: print('class counts (train):', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f68fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Trainer 已增强：LabelSmoothing + Mixup + TTA\n"
     ]
    }
   ],
   "source": [
    "# 舍弃：仅label smoothing+mixup+tta\n",
    "# === 覆盖 Trainer：Label Smoothing + Mixup + TTA ===\n",
    "'''from torch.amp import autocast, GradScaler\n",
    "\n",
    "def _label_smoothing_ce(logits, targets, smoothing=0.1):\n",
    "    if smoothing <= 0:\n",
    "        return F.cross_entropy(logits, targets)\n",
    "    n_class = logits.size(1)\n",
    "    log_prob = F.log_softmax(logits, dim=1)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(log_prob)\n",
    "        true_dist.fill_(smoothing / (n_class - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1 - smoothing)\n",
    "    return torch.mean(torch.sum(-true_dist * log_prob, dim=1))\n",
    "\n",
    "AMP_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def _label_smoothing_ce(logits, targets, smoothing=0.1, weight=None):\n",
    "    n = logits.size(1)\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logp)\n",
    "        true_dist.fill_(smoothing / (n - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1 - smoothing)\n",
    "    loss = torch.sum(-true_dist * logp, dim=1)       # [B]\n",
    "    if weight is not None:\n",
    "        loss = loss * weight[targets]                # 类加权\n",
    "    return loss.mean()\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, lr=2e-4, weight_decay=5e-4, mixup_alpha=0.2, tta_times=4):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #self.scaler = GradScaler(DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.scaler = GradScaler(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.tta_times = tta_times\n",
    "\n",
    "    def _mixup(self, x, y, alpha):\n",
    "        if alpha <= 0:\n",
    "            return x, y, 1.0\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        idx = torch.randperm(x.size(0), device=x.device)\n",
    "        x_mix = lam * x + (1 - lam) * x[idx]\n",
    "        return x_mix, (y, y[idx]), lam\n",
    "\n",
    "    def _tta_predict(self, imgs, labels):\n",
    "        # 简单 TTA：水平翻转 + 旋转 90/270\n",
    "        logits_sum = 0\n",
    "        imgs_tta = [imgs,\n",
    "                    torch.flip(imgs, dims=[-1]),\n",
    "                    imgs.transpose(-1, -2),\n",
    "                    torch.flip(imgs.transpose(-1, -2), dims=[-1])]\n",
    "        with torch.no_grad():\n",
    "            for im in imgs_tta[:self.tta_times]:\n",
    "                out = self.model(im, labels=labels, mode='eval')\n",
    "                if isinstance(out, tuple) and len(out)==2:\n",
    "                    _, logits = out\n",
    "                else:\n",
    "                    # Baseline/Innov1 eval 都返回 (res_stack/logits)\n",
    "                    _, logits = out\n",
    "                logits_sum = logits_sum + logits\n",
    "        return logits_sum / len(imgs_tta[:self.tta_times])\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            self.optim.zero_grad(set_to_none=True)\n",
    "            # Mixup\n",
    "            imgs_mix, ys_tuple, lam = self._mixup(imgs, ys, self.mixup_alpha)\n",
    "            #with autocast(DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "            with autocast(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "                out = self.model(imgs_mix, labels=ys, mode='train')\n",
    "                if isinstance(out, tuple):\n",
    "                    pred_noise, noise, t, _ = out\n",
    "                    diffusion_loss = self.mse(pred_noise, noise)\n",
    "                else:\n",
    "                    diffusion_loss = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "                        res = self.model.reconstruct_residual_fast(imgs_mix, ys, t_start=200, steps=6)\n",
    "                    else:\n",
    "                        res = MultiScaleResidualExtractor(self.model.unet, TIMESTEPS_SAFE).extract_one_timestep(imgs_mix, ys, t_value=200, steps=6)\n",
    "                if hasattr(self.model, 'fusion_classifier'):\n",
    "                    logits = self.model.fusion_classifier(res.unsqueeze(1).to(next(self.model.fusion_classifier.parameters()).device))\n",
    "                else:\n",
    "                    logits = self.model.classifier(res)\n",
    "\n",
    "                if isinstance(ys_tuple, tuple):\n",
    "                    y1, y2 = ys_tuple\n",
    "                    ce = lam * _label_smoothing_ce(logits, y1, smoothing=0.1) + (1-lam) * _label_smoothing_ce(logits, y2, smoothing=0.1)\n",
    "\n",
    "                    # 若有 mixup:\n",
    "                    ce = lam * _label_smoothing_ce(logits, y1, 0.1, CLASS_WEIGHTS_TENSOR)+ (1-lam) * _label_smoothing_ce(logits, y2, 0.1, CLASS_WEIGHTS_TENSOR)\n",
    "                else:\n",
    "                    #ce = _label_smoothing_ce(logits, ys, smoothing=0.1)\n",
    "                    ce = _label_smoothing_ce(logits, ys, smoothing=0.1, weight=CLASS_WEIGHTS_TENSOR)\n",
    "                loss = diffusion_loss + 0.5 * ce\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                total += ys.size(0)\n",
    "                loss_sum += loss.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            logits = self._tta_predict(imgs, ys)\n",
    "            #ce = F.cross_entropy(logits, ys)\n",
    "            ce = F.cross_entropy(logits, ys, weight=CLASS_WEIGHTS_TENSOR)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==ys).sum().item()\n",
    "            total += ys.size(0)\n",
    "            loss_sum += ce.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "print('[Info] Trainer 已增强：LabelSmoothing + Mixup + TTA')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a1ddd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#新增：Mask + Triple Constraints (对比/原型/一致性)，但无LabelSmoothing + Mixup + TTA\n",
    "# === 覆盖 Trainer：Mask + Triple Constraints (对比/原型/一致性) ===\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# —— 组件：投影头、监督对比、原型库、特征钩子 —— #\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=128, hidden=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, h):\n",
    "        return F.normalize(self.net(h), dim=-1)\n",
    "\n",
    "\n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.tau = temperature\n",
    "    def forward(self, z, y):\n",
    "        sim = z @ z.t() / self.tau\n",
    "        sim = sim - torch.eye(sim.size(0), device=sim.device) * 1e9\n",
    "        y1, y2 = y.unsqueeze(1), y.unsqueeze(0)\n",
    "        pos_mask = (y1 == y2).float() - torch.eye(y.size(0), device=y.device)\n",
    "        log_prob = sim - torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "        pos_log = (pos_mask * log_prob).sum(1) / (pos_mask.sum(1) + 1e-9)\n",
    "        return -pos_log.mean()\n",
    "\n",
    "\n",
    "class ProtoBank(nn.Module):\n",
    "    \"\"\"维护类中心 μ_c（EMA）；残差 r = h - μ_y\"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.m = momentum\n",
    "        self.register_buffer(\"mu\", torch.zeros(num_classes, feat_dim))\n",
    "        self.register_buffer(\"cnt\", torch.zeros(num_classes))\n",
    "    @torch.no_grad()\n",
    "    def update(self, h, y):\n",
    "        for c in y.unique():\n",
    "            c = int(c.item())\n",
    "            msk = (y == c)\n",
    "            if msk.sum() == 0: continue\n",
    "            mean_c = h[msk].mean(0)\n",
    "            self.mu[c] = self.m * self.mu[c] + (1 - self.m) * mean_c\n",
    "            self.cnt[c] += msk.sum()\n",
    "    def residual(self, h, y):\n",
    "        return h - self.mu[y]\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 分类特征钩子（不改你模型结构）\n",
    "# ============================\n",
    "class _FeatHook:\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.buffer = None\n",
    "        self.h = module.register_forward_hook(self._hook)\n",
    "    def _hook(self, mod, inp, out):\n",
    "        # 对 Linear：inp[0] 即上一层输出（我们要的 h）\n",
    "        self.buffer = inp[0].detach()\n",
    "    def close(self):\n",
    "        self.h.remove()\n",
    "\n",
    "\n",
    "def _find_last_linear(m: nn.Module):\n",
    "    last_lin = None\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            last_lin = mod\n",
    "    return last_lin\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Trainer（掩膜 + 三重约束；训练禁用LS/Mixup；TTA仅验证）\n",
    "# ============================\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, lr=2e-4, weight_decay=5e-4,\n",
    "                 use_mask_and_triple=True,\n",
    "                 proj_dim=128, contrast_tau=0.2,\n",
    "                 lambda_contrast=0.2, lambda_proto=0.1, lambda_mv=0.1,\n",
    "                 mv_on_logits=True,\n",
    "                 tta_times=2):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.use_mask_and_triple = use_mask_and_triple\n",
    "        self.mv_on_logits = mv_on_logits\n",
    "        self.lambda_contrast = lambda_contrast\n",
    "        self.lambda_proto = lambda_proto\n",
    "        self.lambda_mv = lambda_mv\n",
    "        self.tta_times = tta_times\n",
    "\n",
    "        # —— 三重约束相关初始化 —— #\n",
    "        self._hook = None\n",
    "        self.proj = None\n",
    "        self.supcon = None\n",
    "        self.protos = None\n",
    "\n",
    "        if self.use_mask_and_triple:\n",
    "            # 找到分类模块（fusion_classifier 优先）\n",
    "            cls_mod = getattr(self.model, 'fusion_classifier', None)\n",
    "            if cls_mod is None:\n",
    "                cls_mod = getattr(self.model, 'classifier', None)\n",
    "            assert cls_mod is not None, \"未找到分类模块（classifier / fusion_classifier）\"\n",
    "\n",
    "            last_lin = _find_last_linear(cls_mod)\n",
    "            assert last_lin is not None, \"分类器内未找到 Linear 层用于抓取特征\"\n",
    "\n",
    "            self._hook = _FeatHook(last_lin)\n",
    "            feat_dim = last_lin.in_features\n",
    "            num_classes = last_lin.out_features\n",
    "\n",
    "            self.proj = ProjectionHead(feat_dim, proj_dim, max(256, proj_dim*4)).to(DEVICE)\n",
    "            self.supcon = SupervisedContrastiveLoss(contrast_tau).to(DEVICE)\n",
    "            self.protos = ProtoBank(num_classes, feat_dim, momentum=0.9).to(DEVICE)\n",
    "\n",
    "        # 残差提取器缓存\n",
    "        self._res_extractor = None\n",
    "\n",
    "    # —— 自适应创建 MultiScaleResidualExtractor，兼容不同签名 —— #\n",
    "    def _get_extractor(self):\n",
    "        if self._res_extractor is not None:\n",
    "            return self._res_extractor\n",
    "        # 如果有快速接口，直接返回 None（外部不使用）\n",
    "        if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "            self._res_extractor = None\n",
    "            return None\n",
    "\n",
    "        # 尝试导入\n",
    "        try:\n",
    "            MRE = MultiScaleResidualExtractor  # 已在你脚本中定义/导入\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"未找到 MultiScaleResidualExtractor，请确认已定义/导入。\")\n",
    "\n",
    "        unet = getattr(self.model, 'unet', None)\n",
    "        if unet is None:\n",
    "            raise RuntimeError(\"model.unet 不存在，无法构建 MultiScaleResidualExtractor。\")\n",
    "\n",
    "        # 尝试不同签名\n",
    "        tried = []\n",
    "        for args in [\n",
    "            (unet,),  # 仅 unet\n",
    "            (unet, globals().get(\"TIMESTEPS_SAFE\", None)),\n",
    "            (unet, globals().get(\"SAFE_MAX_PIXELS\", None)),\n",
    "        ]:\n",
    "            # 去除 None\n",
    "            args = tuple(a for a in args if a is not None)\n",
    "            try:\n",
    "                extractor = MRE(*args)\n",
    "                self._res_extractor = extractor\n",
    "                return extractor\n",
    "            except TypeError as e:\n",
    "                tried.append((args, str(e)))\n",
    "                continue\n",
    "\n",
    "        # 如果都失败，抛出更清晰的错误\n",
    "        msg = \"MultiScaleResidualExtractor 构造失败，尝试的签名如下：\\n\"\n",
    "        for a, e in tried:\n",
    "            msg += f\"  - {a}: {e}\\n\"\n",
    "        raise TypeError(msg)\n",
    "\n",
    "    def _forward_classify_logits_and_feat(self, residual_tensor):\n",
    "        \"\"\"前向分类，返回 (logits, h)，h 为最后线性层输入（hook获取）\"\"\"\n",
    "        if hasattr(self.model, 'fusion_classifier'):\n",
    "            logits = self.model.fusion_classifier(residual_tensor.unsqueeze(1).to(next(self.model.fusion_classifier.parameters()).device))\n",
    "        else:\n",
    "            logits = self.model.classifier(residual_tensor)\n",
    "        h = self._hook.buffer if (self._hook is not None and self._hook.buffer is not None) else None\n",
    "        return logits, h\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        训练：loader 返回 (x1, x2, y)\n",
    "          - x1: 常规视图\n",
    "          - x2: 掩膜视图\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "        for batch in loader:\n",
    "            # 兼容旧版 (x, y)\n",
    "            if self.use_mask_and_triple and len(batch) == 3:\n",
    "                x1, x2, ys = batch\n",
    "            else:\n",
    "                x1, ys = batch\n",
    "                x2 = x1\n",
    "\n",
    "            x1 = x1.to(DEVICE); x2 = x2.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                # —— 扩散 MSE —— #\n",
    "                out = self.model(x1, labels=ys, mode='train')\n",
    "                if isinstance(out, tuple):\n",
    "                    pred_noise, noise, t, _ = out\n",
    "                    diffusion_loss = self.mse(pred_noise, noise)\n",
    "                else:\n",
    "                    diffusion_loss = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "                # —— 残差视图（无梯度） —— #\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "                        res1 = self.model.reconstruct_residual_fast(x1, ys, t_start=200, steps=8)\n",
    "                        res2 = self.model.reconstruct_residual_fast(x2, ys, t_start=200, steps=8)\n",
    "                    else:\n",
    "                        extractor = self._get_extractor()\n",
    "                        # 兼容你项目中的 API：extract_one_timestep(x, ys, t_value=..., steps=...)\n",
    "                        res1 = extractor.extract_one_timestep(x1, ys, t_value=200, steps=8)\n",
    "                        res2 = extractor.extract_one_timestep(x2, ys, t_value=200, steps=8)\n",
    "\n",
    "                # —— 分类 + 特征 —— #\n",
    "                logits1, h1 = self._forward_classify_logits_and_feat(res1)\n",
    "                logits2, h2 = self._forward_classify_logits_and_feat(res2)\n",
    "\n",
    "                ce = self.ce(logits1, ys)\n",
    "\n",
    "                if not self.use_mask_and_triple:\n",
    "                    loss = diffusion_loss + 0.5 * ce\n",
    "                else:\n",
    "                    # 对比\n",
    "                    z1 = self.proj(h1); z2 = self.proj(h2)\n",
    "                    zcat = torch.cat([z1, z2], 0)\n",
    "                    ycat = torch.cat([ys, ys], 0)\n",
    "                    loss_con = self.supcon(zcat, ycat)\n",
    "\n",
    "                    # 原型残差\n",
    "                    with torch.no_grad():\n",
    "                        self.protos.update(h1.detach(), ys)\n",
    "                        self.protos.update(h2.detach(), ys)\n",
    "                    r1 = self.protos.residual(h1, ys)\n",
    "                    r2 = self.protos.residual(h2, ys)\n",
    "                    loss_proto = 0.5 * (r1.norm(dim=1).mean() + r2.norm(dim=1).mean())\n",
    "\n",
    "                    # 多视角一致性\n",
    "                    if self.mv_on_logits:\n",
    "                        p1 = F.log_softmax(logits1, dim=1)\n",
    "                        p2 = F.softmax(logits2, dim=1)\n",
    "                        loss_mv = 0.5 * (\n",
    "                            F.kl_div(p1, p2, reduction=\"batchmean\") +\n",
    "                            F.kl_div(F.log_softmax(logits2, dim=1), F.softmax(logits1, dim=1), reduction=\"batchmean\")\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_mv = F.mse_loss(r1, r2)\n",
    "\n",
    "                    loss = diffusion_loss + 0.5 * ce + \\\n",
    "                           self.lambda_contrast * loss_con + \\\n",
    "                           self.lambda_proto     * loss_proto + \\\n",
    "                           self.lambda_mv        * loss_mv\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits1.argmax(1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                total   += ys.size(0)\n",
    "                loss_sum += loss.item() * ys.size(0)\n",
    "\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, loader):\n",
    "        \"\"\"验证：允许 TTA，仅在验证；指标统一硬标签\"\"\"\n",
    "        self.model.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            # 你的 eval 路径（返回 res_stack, logits）\n",
    "            res_stack, logits = self.model(imgs, labels=ys, mode='eval')\n",
    "            ce = self.ce(logits, ys)\n",
    "\n",
    "            # 简单 TTA（水平翻转）；若你已有更完善的 TTA 可替换\n",
    "            if self.tta_times and self.tta_times > 1:\n",
    "                logits_flip = self.model(torch.flip(imgs, dims=[-1]), labels=ys, mode='eval')[1]\n",
    "                logits = 0.5 * (logits + logits_flip)\n",
    "\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == ys).sum().item()\n",
    "            total   += ys.size(0)\n",
    "            loss_sum += ce.item() * ys.size(0)\n",
    "\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    def close(self):\n",
    "        if self._hook is not None:\n",
    "            self._hook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffbeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "949e065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineFaultDetector params(M): 25.453033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = 'baseline'  # 可选：'baseline' | 'innov1' | 'innov2'\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    model = BaselineFaultDetector(num_classes=len(CLASS_NAMES))\n",
    "elif model_type == 'innov1':\n",
    "    model = Innovation1FaultDetector(num_classes=len(CLASS_NAMES))\n",
    "else:\n",
    "    model = Innovation2FaultDetector(num_classes=len(CLASS_NAMES),\n",
    "                                     use_transformer=False,\n",
    "                                     timesteps=[200],\n",
    "                                     lightweight=True)\n",
    "\n",
    "print(model.__class__.__name__, 'params(M):', sum(p.numel() for p in model.parameters())/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1028/1549865020.py:87: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
      "/tmp/ipykernel_1028/1549865020.py:196: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 56.5191 acc 0.2766 | val loss 2.4407 acc 0.3390\n",
      "[SAVE] best acc 0.3390 -> ./substation_part2_best.pt\n",
      "Epoch 02 | train loss 2.6589 acc 0.3957 | val loss 1.7470 acc 0.4379\n",
      "[SAVE] best acc 0.4379 -> ./substation_part2_best.pt\n",
      "Epoch 03 | train loss 2.5775 acc 0.4996 | val loss 1.4958 acc 0.4962\n",
      "[SAVE] best acc 0.4962 -> ./substation_part2_best.pt\n",
      "Epoch 04 | train loss 2.5351 acc 0.5579 | val loss 1.3055 acc 0.5445\n",
      "[SAVE] best acc 0.5445 -> ./substation_part2_best.pt\n",
      "Epoch 05 | train loss 2.5031 acc 0.6054 | val loss 1.2718 acc 0.5928\n",
      "[SAVE] best acc 0.5928 -> ./substation_part2_best.pt\n",
      "Epoch 06 | train loss 2.4752 acc 0.6070 | val loss 1.2695 acc 0.5974\n",
      "[SAVE] best acc 0.5974 -> ./substation_part2_best.pt\n",
      "Epoch 07 | train loss 2.4795 acc 0.6568 | val loss 1.1247 acc 0.6219\n",
      "[SAVE] best acc 0.6219 -> ./substation_part2_best.pt\n",
      "Epoch 08 | train loss 2.4533 acc 0.6513 | val loss 1.1360 acc 0.6120\n",
      "Epoch 09 | train loss 2.4296 acc 0.6595 | val loss 1.1585 acc 0.6196\n"
     ]
    }
   ],
   "source": [
    "# baseline 模型\n",
    "\n",
    "epochs = 50\n",
    "#trainer = Trainer(model, lr=2e-4, weight_decay=5e-4)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    lr=2e-4,\n",
    "    weight_decay=5e-4,\n",
    "    use_mask_and_triple=True,   # ← 开启掩膜 + 三重约束\n",
    "    proj_dim=128,\n",
    "    contrast_tau=0.2,\n",
    "    lambda_contrast=0.2,\n",
    "    lambda_proto=0.1,\n",
    "    lambda_mv=0.1,\n",
    "    mv_on_logits=True,          # 若想约束“残差一致性”，设为 False\n",
    "    tta_times=2                 # 仅验证用；训练阶段不用TTA\n",
    ")\n",
    "\n",
    "best_acc, best_state = 0.0, None\n",
    "save_path = './substation_part2_best.pt'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = trainer.train_epoch(train_loader)\n",
    "    va_loss, va_acc = trainer.validate(val_loader)\n",
    "    print(f'Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}')\n",
    "\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        best_state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': best_acc,\n",
    "            'classes': list(CLASS_NAMES),\n",
    "            'img_size': img_size\n",
    "        }\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f'[SAVE] best acc {best_acc:.4f} -> {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce00368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26225/3365978892.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./substation_part2_best.pt', map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint with acc: 0.6265337423312883\n",
      "Final Val -> loss: 1.7861 acc: 0.6212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('./substation_part2_best.pt'):\n",
    "    ckpt = torch.load('./substation_part2_best.pt', map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    print('Loaded best checkpoint with acc:', ckpt.get('acc', None))\n",
    "else:\n",
    "    print('No checkpoint found, using current model.')\n",
    "\n",
    "va_loss, va_acc = Trainer(model).validate(val_loader)\n",
    "print('Final Val -> loss:', round(va_loss,4), 'acc:', round(va_acc,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6afe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovation1FaultDetector params(M): 25.584446\n"
     ]
    }
   ],
   "source": [
    "# innov1 模型\n",
    "\n",
    "model_type = 'innov1'  # 可选：'baseline' | 'innov1' | 'innov2'\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    model = BaselineFaultDetector(num_classes=len(CLASS_NAMES))\n",
    "elif model_type == 'innov1':\n",
    "    model = Innovation1FaultDetector(num_classes=len(CLASS_NAMES))\n",
    "else:\n",
    "    model = Innovation2FaultDetector(num_classes=len(CLASS_NAMES),\n",
    "                                     use_transformer=False,\n",
    "                                     timesteps=[200],\n",
    "                                     lightweight=True)\n",
    "\n",
    "print(model.__class__.__name__, 'params(M):', sum(p.numel() for p in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15857e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 32.6087 acc 0.1729 | val loss 2.1786 acc 0.1956\n",
      "[SAVE] best acc 0.1956 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 02 | train loss 1.0781 acc 0.2872 | val loss 2.1522 acc 0.2684\n",
      "[SAVE] best acc 0.2684 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 03 | train loss 0.9903 acc 0.3390 | val loss 2.0759 acc 0.3029\n",
      "[SAVE] best acc 0.3029 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 04 | train loss 0.8831 acc 0.4191 | val loss 1.9939 acc 0.3459\n",
      "[SAVE] best acc 0.3459 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 05 | train loss 0.8128 acc 0.4607 | val loss 1.8152 acc 0.4080\n",
      "[SAVE] best acc 0.4080 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 06 | train loss 0.7615 acc 0.4958 | val loss 1.6276 acc 0.4440\n",
      "[SAVE] best acc 0.4440 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 07 | train loss 0.7040 acc 0.5322 | val loss 1.7313 acc 0.4555\n",
      "[SAVE] best acc 0.4555 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 08 | train loss 0.6706 acc 0.5516 | val loss 1.6666 acc 0.4601\n",
      "[SAVE] best acc 0.4601 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 09 | train loss 0.6444 acc 0.5709 | val loss 1.4529 acc 0.5146\n",
      "[SAVE] best acc 0.5146 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 10 | train loss 0.6249 acc 0.5824 | val loss 1.4558 acc 0.5123\n",
      "Epoch 11 | train loss 0.6032 acc 0.6026 | val loss 1.7474 acc 0.4494\n",
      "Epoch 12 | train loss 0.5883 acc 0.6108 | val loss 1.6127 acc 0.4862\n",
      "Epoch 13 | train loss 0.5576 acc 0.6331 | val loss 1.2590 acc 0.5874\n",
      "[SAVE] best acc 0.5874 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 14 | train loss 0.5279 acc 0.6446 | val loss 1.3982 acc 0.5621\n",
      "Epoch 15 | train loss 0.5236 acc 0.6530 | val loss 1.5616 acc 0.4831\n",
      "Epoch 16 | train loss 0.5051 acc 0.6674 | val loss 1.7332 acc 0.5008\n",
      "Epoch 17 | train loss 0.4987 acc 0.6764 | val loss 1.4407 acc 0.5322\n",
      "Epoch 18 | train loss 0.4759 acc 0.6816 | val loss 2.0189 acc 0.4609\n",
      "Epoch 19 | train loss 0.4783 acc 0.6825 | val loss 1.2806 acc 0.5913\n",
      "[SAVE] best acc 0.5913 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 20 | train loss 0.4394 acc 0.7051 | val loss 1.5886 acc 0.5452\n",
      "Epoch 21 | train loss 0.4590 acc 0.6944 | val loss 1.4036 acc 0.5775\n",
      "Epoch 22 | train loss 0.4313 acc 0.7176 | val loss 1.7214 acc 0.4877\n",
      "Epoch 23 | train loss 0.4201 acc 0.7245 | val loss 1.4294 acc 0.5613\n",
      "Epoch 24 | train loss 0.4205 acc 0.7230 | val loss 1.4487 acc 0.5475\n",
      "Epoch 25 | train loss 0.4131 acc 0.7304 | val loss 1.3666 acc 0.5982\n",
      "[SAVE] best acc 0.5982 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 26 | train loss 0.3829 acc 0.7437 | val loss 1.4332 acc 0.5974\n",
      "Epoch 27 | train loss 0.3855 acc 0.7515 | val loss 1.3812 acc 0.5913\n",
      "Epoch 28 | train loss 0.3905 acc 0.7389 | val loss 1.4259 acc 0.5920\n",
      "Epoch 29 | train loss 0.3517 acc 0.7745 | val loss 1.4512 acc 0.5936\n",
      "Epoch 30 | train loss 0.3573 acc 0.7669 | val loss 1.3977 acc 0.6074\n",
      "[SAVE] best acc 0.6074 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 31 | train loss 0.3386 acc 0.7826 | val loss 1.6177 acc 0.5690\n",
      "Epoch 32 | train loss 0.3179 acc 0.7916 | val loss 1.5678 acc 0.6081\n",
      "[SAVE] best acc 0.6081 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 33 | train loss 0.3127 acc 0.7912 | val loss 1.5655 acc 0.5989\n",
      "Epoch 34 | train loss 0.3203 acc 0.7964 | val loss 1.4745 acc 0.6334\n",
      "[SAVE] best acc 0.6334 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 35 | train loss 0.2925 acc 0.8127 | val loss 1.4286 acc 0.6342\n",
      "[SAVE] best acc 0.6342 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 36 | train loss 0.2923 acc 0.8085 | val loss 1.4498 acc 0.6219\n",
      "Epoch 37 | train loss 0.2832 acc 0.8121 | val loss 1.4801 acc 0.6074\n",
      "Epoch 38 | train loss 0.2549 acc 0.8368 | val loss 1.4820 acc 0.6319\n",
      "Epoch 39 | train loss 0.2509 acc 0.8382 | val loss 1.4214 acc 0.6511\n",
      "[SAVE] best acc 0.6511 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 40 | train loss 0.2419 acc 0.8455 | val loss 1.5275 acc 0.6334\n",
      "Epoch 41 | train loss 0.2225 acc 0.8645 | val loss 1.4939 acc 0.6557\n",
      "[SAVE] best acc 0.6557 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 42 | train loss 0.2149 acc 0.8618 | val loss 1.6174 acc 0.6112\n",
      "Epoch 43 | train loss 0.2144 acc 0.8671 | val loss 1.6275 acc 0.6311\n",
      "Epoch 44 | train loss 0.2073 acc 0.8715 | val loss 1.8416 acc 0.6158\n",
      "Epoch 45 | train loss 0.2052 acc 0.8723 | val loss 1.8279 acc 0.5836\n",
      "Epoch 46 | train loss 0.2022 acc 0.8694 | val loss 1.6828 acc 0.6334\n",
      "Epoch 47 | train loss 0.1931 acc 0.8752 | val loss 1.5768 acc 0.6587\n",
      "[SAVE] best acc 0.6587 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 48 | train loss 0.1830 acc 0.8857 | val loss 1.8852 acc 0.6081\n",
      "Epoch 49 | train loss 0.1987 acc 0.8783 | val loss 1.7190 acc 0.6534\n",
      "Epoch 50 | train loss 0.1922 acc 0.8794 | val loss 1.6968 acc 0.6411\n"
     ]
    }
   ],
   "source": [
    "# innov1 模型\n",
    "\n",
    "epochs = 50\n",
    "trainer = Trainer(model, lr=2e-4, weight_decay=5e-4)\n",
    "\n",
    "best_acc, best_state = 0.0, None\n",
    "save_path = './innov1_substation_part2_best.pt'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = trainer.train_epoch(train_loader)\n",
    "    va_loss, va_acc = trainer.validate(val_loader)\n",
    "    print(f'Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}')\n",
    "    \n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        best_state = {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                      'acc': best_acc, 'classes': list(CLASS_NAMES), 'img_size': img_size}\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f'[SAVE] best acc {best_acc:.4f} -> {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b23af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26225/1414661540.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./innov1_substation_part2_best.pt', map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint with acc: 0.6587423312883436\n",
      "Final Val -> loss: 1.5816 acc: 0.6426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('./innov1_substation_part2_best.pt'):\n",
    "    ckpt = torch.load('./innov1_substation_part2_best.pt', map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    print('Loaded best checkpoint with acc:', ckpt.get('acc', None))\n",
    "else:\n",
    "    print('No checkpoint found, using current model.')\n",
    "\n",
    "va_loss, va_acc = Trainer(model).validate(val_loader)\n",
    "print('Final Val -> loss:', round(va_loss,4), 'acc:', round(va_acc,4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant-py310-test)",
   "language": "python",
   "name": "plant-py310-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
