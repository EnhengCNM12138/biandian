{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 禁用 Innov2（防误用）===\n",
    "class _Innov2Disabled:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Innov2 已禁用，请仅使用 Baseline 或 Innovation1\")\n",
    "\n",
    "Innovation2FaultDetector = _Innov2Disabled\n",
    "FusionClassifier = _Innov2Disabled\n",
    "TransformerFusionClassifier = _Innov2Disabled\n",
    "print('[Info] Innov2 已禁用：Innovation2FaultDetector / FusionClassifier / TransformerFusionClassifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bf358",
   "metadata": {},
   "source": [
    "\n",
    "# 变电站故障检测\n",
    "- 支持 Baseline / 创新点1 / 创新点2（先以轻量模式稳定跑通）  \n",
    "- 端到端：数据加载 → 训练 → 推理 → 评估  \n",
    "- 已内置 **11 个正确标签**：\n",
    "`'bj_bpmh', 'bj_bpps', 'bj_wkps', 'bjdsye', 'jyz_pl', 'sly_dmyw', 'hxg_gjbs', 'hxq_gjtps', 'xmbhyc', 'yw_gkxfw', 'yw_nc'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dcc0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | Classes: 11\n"
     ]
    }
   ],
   "source": [
    "import os, math, time, json, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if CUDA else 'cpu')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if CUDA: torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SAFE_MAX_PIXELS = 384 * 384   # 安全调整size\n",
    "USE_FP16 = True               # 启用amp\n",
    "TIMESTEPS_SAFE = [200]        # 开启稳定single-timestep\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'bj_bpmh', 'bj_bpps', 'bj_wkps', 'bjdsyc', 'jyz_pl', 'sly_dmyw',\n",
    "    'hxq_gjbs', 'hxq_gjtps', 'xmbhyc', 'yw_gkxfw', 'yw_nc'\n",
    "]\n",
    "print('Device:', DEVICE, '| Classes:', len(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caff8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：将 4D 图像张量（B,C,H,W）安全缩放到不超过 SAFE_MAX_PIXELS 的像素上限，避免显存暴涨。\n",
    "\n",
    "def safe_resize_bchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    B,C,H,W = x.shape\n",
    "    if H*W <= SAFE_MAX_PIXELS:\n",
    "        return x\n",
    "    scale = (SAFE_MAX_PIXELS / float(H*W)) ** 0.5\n",
    "    newH, newW = max(64, int(H*scale)), max(64, int(W*scale))\n",
    "    return F.interpolate(x, size=(newH,newW), mode='bilinear', align_corners=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f1e76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 已启用更强数据增强与标准化\n"
     ]
    }
   ],
   "source": [
    "#功能：解析单个 VOC 风格 XML，提取类别名\n",
    "\n",
    "def parse_xml(xml_path: str) -> Optional[str]:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text.strip()\n",
    "            return name\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "#功能：自定义数据集类，从指定目录加载图像-标签对\n",
    "class SubstationDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, annos_dir: str, transform=None,\n",
    "                 class_names: List[str] = None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.annos_dir = Path(annos_dir)\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names or list(CLASS_NAMES)\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
    "        self.data_pairs = []\n",
    "        exts = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
    "        annos = [p for p in self.annos_dir.iterdir() if p.suffix.lower()=='.xml']\n",
    "        for ap in annos:\n",
    "            label = parse_xml(str(ap))\n",
    "            if label is None: \n",
    "                continue\n",
    "            if label not in self.class_to_idx:\n",
    "                continue\n",
    "            img_stem = ap.stem\n",
    "            img_candidate = None\n",
    "            for ext in exts:\n",
    "                ip = self.images_dir / f\"{img_stem}{ext}\"\n",
    "                if ip.exists():\n",
    "                    img_candidate = ip; break\n",
    "            if img_candidate is None:\n",
    "                for p in self.images_dir.iterdir():\n",
    "                    if p.suffix.lower() in exts and p.stem == img_stem:\n",
    "                        img_candidate = p; break\n",
    "            if img_candidate is not None:\n",
    "                self.data_pairs.append( (str(img_candidate), self.class_to_idx[label]) )\n",
    "        if len(self.data_pairs)==0:\n",
    "            print(\"[WARN] No matched image-xml pairs found.\")\n",
    "\n",
    "    def __len__(self): return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip, y = self.data_pairs[idx]\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "#功能：返回训练/验证两套 torchvision.transforms 变换\n",
    "#训练：随机水平/垂直翻转、颜色抖动、缩放、ToTensor\n",
    "#验证：缩放、ToTensor\n",
    "'''def get_transforms(img_size=256):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    return train_tf, val_tf'''\n",
    "\n",
    "    # === 更强数据增强（减轻过拟合）===\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transforms(img_size=256):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(0.3,0.3,0.3,0.15)], p=0.8),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 1.5))], p=0.3),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.14), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "print('[Info] 已启用更强数据增强与标准化')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5dd927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：扩散过程类，包含 q_sample 和 p_sample 方法\n",
    "#定义离散扩散过程的时间表与前向/反向采样公式（DDPM 公式族）\n",
    "class DiffusionProcess(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.0)\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "# 功能：前向采样，生成带噪声的图像(前向加噪)\n",
    "# 输入：原始图像 x_start, 时间步 t, 可选噪声 noise\n",
    "# 输出：带噪声的图像 x_noisy\n",
    "# 实现：根据时间步 t 的 alpha 和 beta 计算加噪强度，生成带噪声的图像\n",
    "# 关键点：用 sqrt(ᾱ_t)、sqrt(1-ᾱ_t) 线性混合 x_start 与 noise\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        acp = self.alphas_cumprod.to(t.device)\n",
    "        sqrt_acp = torch.sqrt(acp.gather(0, t)).view(-1,1,1,1)\n",
    "        sqrt_om = torch.sqrt((1.0 - acp).gather(0, t)).view(-1,1,1,1)\n",
    "        return sqrt_acp * x_start + sqrt_om * noise\n",
    "\n",
    "# 功能：反向采样，从带噪声的图像恢复原始图像(反向去噪,调用 U-Net 预测噪声,支持类条件传递)\n",
    "# 输入：U-Net 模型, 带噪声的图像 x, 时间步 t, 时间步索引 t_index, 可选类条件标签 class_labels\n",
    "# 输出：恢复的原始图像\n",
    "# 实现：根据时间步 t 的 alpha 和 beta 计算去噪强度，结合 U-Net 预测的噪声，恢复原始图像\n",
    "# 关键点：用 sqrt(ᾱ_t)、sqrt(1-ᾱ_t) 线性混合 x 与 U-Net 预测的噪声(类条件会被透传到去噪器)\n",
    "    def p_sample(self, model, x, t, t_index, class_labels=None):\n",
    "        betas_t = self.betas.to(t.device).gather(0, t).view(-1,1,1,1)\n",
    "        acp_t = self.alphas_cumprod.to(t.device).gather(0, t)\n",
    "        alphas_t = self.alphas.to(t.device).gather(0, t)\n",
    "        sqrt_one_minus = torch.sqrt(1.0 - acp_t).view(-1,1,1,1)\n",
    "        sqrt_recip = torch.sqrt(1.0 / alphas_t).view(-1,1,1,1)\n",
    "        pred_noise = model(x, t, class_labels=class_labels)\n",
    "        model_mean = sqrt_recip * (x - betas_t * pred_noise / sqrt_one_minus)\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        post_var_t = self.posterior_variance.to(t.device).gather(0, t).view(-1,1,1,1)\n",
    "        return model_mean + torch.sqrt(post_var_t) * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82a2e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：U-Net 的残差块，包含时间条件、类条件传递、短连接等.\n",
    "#输入：输入特征 x, 时间嵌入 t_emb, 可选类条件 cond\n",
    "#输出：输出特征 h,(与 out_ch 匹配的特征图)\n",
    "#实现：\n",
    "# 1) 两个 3x3 卷积 + 时间条件 + 类条件传递\n",
    "# 2) 短连接：如果输入输出通道数相同，则直接相加；否则用 1x1 卷积调整通道数\n",
    "# 3) 使用 SiLU 激活函数，可选 Dropout 正则化\n",
    "class ResidBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, cond_dim=None, p=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.short = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.use_film = cond_dim is not None\n",
    "        if self.use_film:\n",
    "            self.gamma = nn.Linear(cond_dim, out_ch)\n",
    "            self.beta  = nn.Linear(cond_dim, out_ch)\n",
    "\n",
    "    def forward(self, x, t_emb, cond=None):\n",
    "        h = self.conv1(x)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        if self.use_film:\n",
    "            gamma = self.gamma(cond)[:, :, None, None]\n",
    "            beta  = self.beta(cond)[:, :, None, None]\n",
    "            h = h * (1 + gamma) + beta\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.short(x)\n",
    "\n",
    "#功能：增强 U-Net 模型,扩散去噪器（U-Net），支持时间条件、类条件传递、短连接等.\n",
    "#输入：输入特征 x, 时间步 timestep, 可选类条件 class_labels\n",
    "#输出：预测的噪声(与输入图像同尺寸的 3 通道张量)\n",
    "#实现：\n",
    "# 1) 时间条件：将时间步转换为嵌入，与特征图拼接\n",
    "# 2) 类条件：如果提供，则转换为可学习的提示，与特征图拼接\n",
    "# 3) 残差块：包含两个 3x3 卷积 + 时间条件 + 类条件传递 + 短连接\n",
    "# 4) 短连接：如果输入输出通道数相同，则直接相加；否则用 1x1 卷积调整通道数\n",
    "# 5) 使用 SiLU 激活函数，可选 Dropout 正则化\n",
    "# 6) 使用 avg_pool2d 下采样，使用 F.interpolate 上采样\n",
    "# 7) 使用 torch.cat 拼接特征图，实现跳跃连接\n",
    "# 8) 使用 nn.Conv2d 输出预测的噪声\n",
    "class EnhancedUNet(nn.Module):\n",
    "    def __init__(self, num_classes: int, base_ch=64, use_film=True, use_attention=False, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        ch = int(base_ch * width_mult)\n",
    "        self.use_film = use_film\n",
    "        self.use_attention = use_attention\n",
    "        time_dim = ch * 4\n",
    "        cond_dim = ch * 2 if use_film else None\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, ch), nn.SiLU(),\n",
    "            nn.Linear(ch, time_dim)\n",
    "        )\n",
    "        self.learnable_prompts = nn.Embedding(num_classes, cond_dim if use_film else 1)\n",
    "\n",
    "        # encoder\n",
    "        self.enc1 = ResidBlock(3,      ch,   time_dim, cond_dim)\n",
    "        self.enc2 = ResidBlock(ch,     ch*2, time_dim, cond_dim)\n",
    "        self.enc3 = ResidBlock(ch*2,   ch*4, time_dim, cond_dim)\n",
    "        self.mid  = ResidBlock(ch*4,   ch*4, time_dim, cond_dim)\n",
    "\n",
    "        # decoder (concat skips)\n",
    "        self.dec3 = ResidBlock(ch*4 + ch*2, ch*2, time_dim, cond_dim)  # 6*ch -> 2*ch\n",
    "        self.dec2 = ResidBlock(ch*2 + ch,   ch,   time_dim, cond_dim)  # 3*ch -> 1*ch\n",
    "        self.dec1 = nn.Conv2d(ch, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, timestep, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            class_labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        t = timestep.view(-1, 1).float()\n",
    "        t_emb = self.time_mlp(t)\n",
    "        cond = self.learnable_prompts(class_labels) if self.use_film else None\n",
    "\n",
    "        # encoder\n",
    "        e1 = self.enc1(x,               t_emb, cond)        # [B, ch,   H,   W]\n",
    "        e2 = self.enc2(F.avg_pool2d(e1, 2), t_emb, cond)    # [B, 2ch,  H/2, W/2]\n",
    "        e3 = self.enc3(F.avg_pool2d(e2, 2), t_emb, cond)    # [B, 4ch,  H/4, W/4]\n",
    "        m  = self.mid(e3,               t_emb, cond)        # [B, 4ch,  H/4, W/4]\n",
    "\n",
    "        # decoder with concat skips\n",
    "        up_m = F.interpolate(m, scale_factor=2, mode='nearest')        # [B, 4ch, H/2, W/2]\n",
    "        #d3_in = torch.cat([up_m, e2], dim=1)                           # [B, 6ch, H/2, W/2]\n",
    "        d3 = self.dec3(torch.cat([up_m, e2], dim=1), t_emb, cond)                             # [B, 2ch, H/2, W/2]\n",
    "\n",
    "        up_d3 = F.interpolate(d3, scale_factor=2, mode='nearest')      # [B, 2ch, H, W]\n",
    "        #d2_in = torch.cat([up_d3, e1], dim=1)                          # [B, 3ch, H, W]\n",
    "        d2 = self.dec2(torch.cat([up_d3, e1], dim=1), t_emb, cond)                              # [B, ch,  H, W]\n",
    "\n",
    "        out = self.dec1(d2)                                            # [B, 3,   H, W]\n",
    "        return out\n",
    "  # predicted noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e8315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差提取与多尺度\n",
    "#功能：多尺度残差提取器，支持多尺度残差提取与扩散去噪.利用扩散反推把输入图像“还原”到接近正常，再与原图取绝对差，得到残差；可按多时间步提取。\n",
    "#输入：扩散去噪器, 时间步列表\n",
    "#输出：多尺度残差列表\n",
    "#实现：\n",
    "# 1) 初始化：保存扩散去噪器与时间步列表\n",
    "# 2) 前向：对每个时间步，调用扩散去噪器进行反向采样，得到残差\n",
    "# 3) 返回：所有时间步的残差列表\n",
    "# 4) 使用 torch.no_grad() 装饰器，避免梯度计算\n",
    "class MultiScaleResidualExtractor(nn.Module):\n",
    "    def __init__(self, diffusion_model: EnhancedUNet, timesteps: List[int]):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.timesteps = timesteps\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "\n",
    "#功能：提取单个时间步的残差,从给定时间步 t_value 反推到 0，得到还原图 x̂ 并计算 |x - x̂|\n",
    "#输入：输入图像 x, 标签 labels, 时间步 t_value, 步数 steps\n",
    "#输出：残差\n",
    "#关键点：每步都携带 class_labels（innov1 的类条件反推）\n",
    "#实现：\n",
    "# 1) 初始化：保存扩散去噪器与时间步列表\n",
    "# 2) 前向：对每个时间步，调用扩散去噪器进行反向采样，得到残差\n",
    "# 3) 返回：所有时间步的残差列表\n",
    "# 被谁调用：验证/推理，多尺度时会被多次调用后堆叠。\n",
    "    @torch.no_grad()\n",
    "    def extract_one_timestep(self, x, labels, t_value: int, steps: int = 8):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev, non_blocking=True)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_value // max(1, steps)))\n",
    "        for i in range(t_value, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        residual = (x - x_t).abs()\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c25d7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] 已覆盖 ResidualClassifier：加入 Dropout 与 LabelSmoothing 支持（由 Trainer 使用）\n"
     ]
    }
   ],
   "source": [
    "#分类器与多尺度融合\n",
    "#功能：残差分类器，将多尺度残差堆叠后，用 ResNet50 提取特征，最后用全连接层分类.对单张残差图做分类。\n",
    "#输入：多尺度残差列表\n",
    "#输出：分类结果\n",
    "#关键点：ResNet50(IMAGENET1K_V2) 作为骨干，只替换最后全连接层输出通道数\n",
    "#实现：\n",
    "# 1) 初始化：加载 ResNet50 模型，替换最后一层全连接为 num_classes 输出\n",
    "# 2) 前向：对每个多尺度残差，用 ResNet50 提取特征，最后用全连接层分类\n",
    "# 3) 返回：分类结果\n",
    "# 被谁调用：验证/推理，多尺度时会被多次调用后堆叠。\n",
    "'''class ResidualClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        in_feat = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_feat, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)'''\n",
    "\n",
    "# === 覆盖 ResidualClassifier：更强正则（Dropout/LabelSmoothing 支持）===\n",
    "class ResidualClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, p_drop=0.4, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        in_feat = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_feat, num_classes)\n",
    "        )\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print('[Info] 已覆盖 ResidualClassifier：加入 Dropout 与 LabelSmoothing 支持（由 Trainer 使用）')\n",
    "\n",
    "\n",
    "\n",
    "#功能：多尺度融合分类器，将多尺度残差堆叠后，用 2 层 3x3 卷积 + 平均池化 + 全连接层分类.\n",
    "#输入：残差栈 (B,T,C,H,W)\n",
    "#关键点：先把 (T,C,H,W) 当“伪通道”或展平做轻量卷积/池化，再接全连接层。\n",
    "#被谁调用：Innov2（多时间步） \n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, num_timesteps: int = 1, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,8))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*8*8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x): # [B,T,C,H,W]\n",
    "        if x.dim()==4:\n",
    "            x = x.unsqueeze(1)\n",
    "        B,T,C,H,W = x.shape\n",
    "        x = x.mean(dim=1)  # avg over time\n",
    "        h = self.conv(x)\n",
    "        h = self.pool(h).view(B,-1)\n",
    "        return self.fc(h)\n",
    "\n",
    "# 功能：Transformer 融合分类器，将多尺度残差堆叠后（把残差栈先做网格池化到固定维度，投影为序列），用 Transformer 提取特征，最后用全连接层分类.\n",
    "#把 (B,T,C,H,W) 先做网格池化到固定维度，投影为序列，再用 TransformerEncoder 做时序/尺度建模。\n",
    "#残差栈 (B,T,C,H,W) → 网格池化 → 序列 (B,T,C*g*g) → TransformerEncoder → 平均池化 (B,D) → 全连接 (B,num_classes)\n",
    "#关键点：每一帧残差提取一个 token，Transformer 编码，做时序平均池化再分类。\n",
    "#被谁调用：Innov2（多时间步 + Transformer 融合）\n",
    "class TransformerFusionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11, num_timesteps=1, in_channels=3, grid=8,\n",
    "                 proj_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.grid = grid\n",
    "        self.proj = nn.Linear(in_channels * grid * grid, proj_dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=proj_dim, nhead=8, dim_feedforward=1024, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(proj_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):  # [B,T,C,H,W]\n",
    "        if x.dim()==4:\n",
    "            x = x.unsqueeze(1)\n",
    "        B,T,C,H,W = x.shape\n",
    "        feats = []\n",
    "        for i in range(T):\n",
    "            f = F.adaptive_avg_pool2d(x[:,i], (self.grid, self.grid))  # [B,C,g,g]\n",
    "            f = f.view(B, -1)                                          # [B, C*g*g]\n",
    "            f = self.proj(f)                                           # [B, D]\n",
    "            feats.append(f)\n",
    "        seq = torch.stack(feats, dim=1)                                # [B,T,D]\n",
    "        enc = self.encoder(seq).mean(dim=1)                            # [B,D]\n",
    "        return self.fc(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bc691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 端到端模型封装\n",
    "# 功能：封装了创新1/2的模型，包括扩散去噪器、多尺度残差提取器、残差分类器、Transformer 融合分类器。\n",
    "# 基线总成：DiffusionProcess + EnhancedUNet(use_film=False) + ResidualClassifier\n",
    "# 输入：图像 x, 标签 labels, 模式 mode\n",
    "# 输出：预测的噪声, 噪声, 时间步, 标签\n",
    "# 关键点：训练时残差支路不回传梯度（通过 no_grad 或单独函数），扩散器只用 MSE 学“正常先验”，分类器用 CE 学“看残差分类”\n",
    "\n",
    "\n",
    "class BaselineFaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11, img_size=256):\n",
    "        super().__init__()\n",
    "        self.diffusion = DiffusionProcess()\n",
    "        self.unet = EnhancedUNet(num_classes=num_classes, use_film=False, width_mult=0.75)\n",
    "        self.classifier = ResidualClassifier(num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = self.diffusion.q_sample(x, t, noise)\n",
    "            pred_noise = self.unet(x_noisy, t, class_labels=None)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                res = MultiScaleResidualExtractor(self.unet, TIMESTEPS_SAFE).extract_one_timestep(\n",
    "                    x, labels if labels is not None else torch.zeros(x.size(0), dtype=torch.long, device=x.device),\n",
    "                    t_value=200, steps=8\n",
    "                )\n",
    "                logits = self.classifier(res)\n",
    "                return res.unsqueeze(1), logits\n",
    "\n",
    "# 创新1总成：DiffusionProcess + EnhancedUNet(use_film=True) + ResidualClassifier\n",
    "# U-Net 开启 use_film=True，并在反推/训练都传入 class_labels\n",
    "# 关键点：FiLM 条件：nn.Embedding(num_classes, cond_dim) 生成类条件向量，注入各残差块；\n",
    "# 反推时也传 labels 给 p_sample，确保“按该类正常形态”还原；\n",
    "# 保持损失组合 MSE + 0.5×CE 与解耦训练不变。\n",
    "\n",
    "class Innovation1FaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "        self.diffusion_model = EnhancedUNet(num_classes=num_classes, use_film=True, width_mult=0.75)\n",
    "        self.classifier = ResidualClassifier(num_classes)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct_residual_fast(self, x, labels, t_start=200, steps=10):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_start // max(1, steps)))\n",
    "        for i in range(t_start, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        return (x - x_t).abs()\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if labels is None:\n",
    "            labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion_process.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = self.diffusion_process.q_sample(x, t, noise)\n",
    "            pred_noise = self.diffusion_model(x_noisy, t, class_labels=labels)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                res = self.reconstruct_residual_fast(x, labels, t_start=200, steps=8)\n",
    "                logits = self.classifier(res)\n",
    "                return res.unsqueeze(1), logits\n",
    "\n",
    "# 创新2总成：DiffusionProcess + EnhancedUNet(use_film=True) + MultiScaleResidualExtractor + FusionClassifier/TransformerFusionClassifier\n",
    "class Innovation2FaultDetector(nn.Module):\n",
    "    def __init__(self, num_classes=11, use_transformer=True, timesteps=[200], lightweight=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.timesteps = timesteps\n",
    "        self.lightweight = lightweight\n",
    "\n",
    "        self.diffusion_process = DiffusionProcess()\n",
    "        self.diffusion_model = EnhancedUNet(num_classes=num_classes, use_film=True,\n",
    "                                            use_attention=(False if lightweight else True),\n",
    "                                            width_mult=(0.75 if lightweight else 1.0))\n",
    "        self.residual_extractor = MultiScaleResidualExtractor(self.diffusion_model, self.timesteps)\n",
    "        if lightweight or not use_transformer:\n",
    "            self.fusion_classifier = FusionClassifier(num_classes=num_classes, num_timesteps=len(self.timesteps))\n",
    "        else:\n",
    "            self.fusion_classifier = TransformerFusionClassifier(num_classes=num_classes, num_timesteps=len(self.timesteps))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct_residual_fast(self, x, labels, t_start=200, steps=8):\n",
    "        dev = next(self.parameters()).device\n",
    "        x = x.to(dev)\n",
    "        x_t = torch.randn_like(x)\n",
    "        stride = max(1, int(t_start // max(1, steps)))\n",
    "        for i in range(t_start, -1, -stride):\n",
    "            tt = torch.full((x.size(0),), i, device=dev, dtype=torch.long)\n",
    "            x_t = self.diffusion_process.p_sample(self.diffusion_model, x_t, tt, i, class_labels=labels.to(dev))\n",
    "        return (x - x_t).abs()\n",
    "\n",
    "    def forward(self, x, labels=None, mode='train'):\n",
    "        if labels is None:\n",
    "            labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        if mode == 'train':\n",
    "            b = x.shape[0]\n",
    "            t = torch.randint(0, self.diffusion_process.num_timesteps, (b,), device=x.device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            masked_x = x\n",
    "            x_noisy = self.diffusion_process.q_sample(masked_x, t, noise)\n",
    "            pred_noise = self.diffusion_model(x_noisy, t, class_labels=labels)\n",
    "            return pred_noise, noise, t, None\n",
    "        else:\n",
    "            self.eval()\n",
    "            dev = next(self.parameters()).device\n",
    "            timesteps = self.timesteps if (self.lightweight is False) else TIMESTEPS_SAFE\n",
    "            with torch.inference_mode():\n",
    "                residual_list = []\n",
    "                for t in timesteps:\n",
    "                    res_t = self.reconstruct_residual_fast(x, labels, t_start=int(t), steps=8)  # [B,C,H,W]\n",
    "                    residual_list.append(res_t)\n",
    "                res_stack = torch.stack(residual_list, dim=1)  # [B,T,C,H,W]\n",
    "                logits = self.fusion_classifier(res_stack.to(next(self.fusion_classifier.parameters()).device))\n",
    "                return res_stack, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ac0da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "AMP_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# 训练器\n",
    "# 功能：训练模型，包括前向、损失计算、反向传播、梯度裁剪、优化器更新等；封装优化器、损失、AMP、单轮训练与验证逻辑、保存最优模型。\n",
    "# 关键点：使用 GradScaler 处理混合精度训练，使用交叉熵损失计算分类误差，使用 MSE 损失计算扩散损失。\n",
    "# 优化器：AdamW；混合精度：GradScaler；\n",
    "# 损失：mse = MSE(pred_noise, noise)；ce = CE(logits, y)；总损失 loss = mse + 0.5*ce；\n",
    "# 保存：当 val_acc 创新高就 torch.save\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, lr=2e-4, weight_decay=5e-4):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #self.scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.scaler = GradScaler(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "#对每个 batch：采样 t 加噪 → U-Net 预测噪声 → MSE；无梯度重构残差 → 分类器 → CE；loss = MSE + 0.5×CE 反传更新；统计训练准确率。\n",
    "# 可选：用 train_eval_loader 在自然分布上再测一次 train_acc。\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            self.optim.zero_grad(set_to_none=True)\n",
    "            #with torch.cuda.amp.autocast(enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "            with autocast(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "                out = self.model(imgs, labels=ys, mode='train')\n",
    "                if isinstance(out, tuple):\n",
    "                    pred_noise, noise, t, _ = out\n",
    "                    diffusion_loss = self.mse(pred_noise, noise)\n",
    "                else:\n",
    "                    diffusion_loss = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "                        res = self.model.reconstruct_residual_fast(imgs, ys, t_start=200, steps=8)\n",
    "                    else:\n",
    "                        res = MultiScaleResidualExtractor(self.model.unet, TIMESTEPS_SAFE).extract_one_timestep(imgs, ys, t_value=200, steps=8)\n",
    "                if hasattr(self.model, 'fusion_classifier'):\n",
    "                    logits = self.model.fusion_classifier(res.unsqueeze(1).to(next(self.model.fusion_classifier.parameters()).device))\n",
    "                else:\n",
    "                    logits = self.model.classifier(res)\n",
    "                ce = self.ce(logits, ys)\n",
    "                loss = diffusion_loss + 0.5 * ce\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                total += ys.size(0)\n",
    "                loss_sum += loss.item() * ys.size(0)\n",
    "\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "# 验证器:验证评估。走推理路径（反推→残差→分类），统计 val_loss/val_ac\n",
    "# 关键点：不做反传；与训练的残差支路保持一致的生成逻辑；遇到更高 acc 即保存 best\n",
    "    @torch.no_grad()\n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            res_stack, logits = self.model(imgs, labels=ys, mode='eval')\n",
    "            ce = self.ce(logits, ys)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==ys).sum().item()\n",
    "            total += ys.size(0)\n",
    "            loss_sum += ce.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb1fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 1) 只构建一次数据集，拿到稳定的 pairs\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m _base \u001b[38;5;241m=\u001b[39m \u001b[43mSubstationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGES_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mANNOTATIONS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCLASS_NAMES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m pairs \u001b[38;5;241m=\u001b[39m _base\u001b[38;5;241m.\u001b[39mdata_pairs  \u001b[38;5;66;03m# [(img_path, class_idx), ...]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pairs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m, in \u001b[0;36mSubstationDataset.__init__\u001b[0;34m(self, images_dir, annos_dir, transform, class_names)\u001b[0m\n\u001b[1;32m     24\u001b[0m annos \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannos_dir\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ap \u001b[38;5;129;01min\u001b[39;00m annos:\n\u001b[0;32m---> 26\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mparse_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m, in \u001b[0;36mparse_xml\u001b[0;34m(xml_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_xml\u001b[39m(xml_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetroot()\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m             name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/plant-py310-test/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m tree \u001b[38;5;241m=\u001b[39m ElementTree()\n\u001b[0;32m-> 1222\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[0;32m~/miniconda3/envs/plant-py310-test/lib/python3.10/xml/etree/ElementTree.py:569\u001b[0m, in \u001b[0;36mElementTree.parse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    567\u001b[0m close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 569\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IMAGES_DIR = '/mnt/e/code/project/Dataset-total/images'   # 修改为你的路径\n",
    "ANNOTATIONS_DIR = '/mnt/e/code/project/Dataset-total/annotations/xmls'  # 修改为你的路径\n",
    "\n",
    "img_size = 256\n",
    "val_ratio = 0.2\n",
    "bs = 8\n",
    "workers = 4\n",
    "\n",
    "'''train_tf, val_tf = get_transforms(img_size)\n",
    "base = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=None, class_names=list(CLASS_NAMES))\n",
    "indices = np.arange(len(base))\n",
    "if len(indices)==0:\n",
    "    raise RuntimeError('No data found. Please check paths.')\n",
    "labels_np = np.array([ base.data_pairs[i][1] for i in indices ])\n",
    "\n",
    "train_idx, val_idx = train_test_split(indices, test_size=val_ratio, random_state=SEED, stratify=labels_np)\n",
    "\n",
    "train_ds = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=train_tf, class_names=list(CLASS_NAMES))\n",
    "val_ds   = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=val_tf,   class_names=list(CLASS_NAMES))\n",
    "train_ds = Subset(train_ds, train_idx.tolist())\n",
    "val_ds   = Subset(val_ds,   val_idx.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) 只构建一次数据集，拿到稳定的 pairs\n",
    "_base = SubstationDataset(IMAGES_DIR, ANNOTATIONS_DIR, transform=None, class_names=list(CLASS_NAMES))\n",
    "pairs = _base.data_pairs  # [(img_path, class_idx), ...]\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    raise RuntimeError('No data found. Check IMAGES_DIR / ANNOTATIONS_DIR.')\n",
    "\n",
    "# 2) 按同一份 pairs 做可复现的划分\n",
    "indices = np.arange(len(pairs))\n",
    "labels_np = np.array([ y for _, y in pairs ])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=val_ratio, random_state=SEED, stratify=labels_np)\n",
    "\n",
    "# 3) 用“同一清单 + 不同 transform”构建 Dataset 封装器，避免重新枚举文件\n",
    "class SubstationDatasetFromPairs(Dataset):\n",
    "    def __init__(self, pairs, transform=None):\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, y = self.pairs[i]\n",
    "        img = Image.open(ip).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "train_tf, val_tf = get_transforms(img_size)\n",
    "train_ds = SubstationDatasetFromPairs([pairs[i] for i in train_idx], transform=train_tf)\n",
    "val_ds   = SubstationDatasetFromPairs([pairs[i] for i in val_idx],   transform=val_tf)\n",
    "\n",
    "# 4) loader：训练用加权采样，评估用自然分布\n",
    "use_weighted_sampler = True\n",
    "\n",
    "if use_weighted_sampler:\n",
    "    train_labels = [ y for _, y in [pairs[i] for i in train_idx] ]\n",
    "    counts = np.bincount(train_labels, minlength=len(CLASS_NAMES))\n",
    "    #class_weights = 1.0 / np.clip(counts, a_min=1, a_max=None)\n",
    "    inv = 1.0 / np.clip(counts, a_min=1, a_max=None)\n",
    "    class_weights_ce = (inv / inv.sum()) * len(inv)          # 归一到均值≈1\n",
    "    CLASS_WEIGHTS_TENSOR = torch.tensor(class_weights_ce, dtype=torch.float32, device=DEVICE)\n",
    "    print('class_weights_ce:', np.round(class_weights_ce, 3))\n",
    "    \n",
    "    sample_weights = [ class_weights_ce[y] for y in train_labels ]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, sampler=sampler, num_workers=workers, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "# 验证集始终用自然分布 + 顺序采样\n",
    "val_loader = DataLoader(val_ds,   batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "# 另外做一个“训练集评估 loader”（自然分布），用于客观的 train_acc\n",
    "train_eval_loader = DataLoader(train_ds, batch_size=max(1,bs//2), shuffle=False, num_workers=workers, pin_memory=True)\n",
    "\n",
    "print('train/val sizes:', len(train_ds), len(val_ds))\n",
    "if use_weighted_sampler: print('class counts (train):', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f68fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Trainer 已增强：LabelSmoothing + Mixup + TTA\n"
     ]
    }
   ],
   "source": [
    "# === 覆盖 Trainer：Label Smoothing + Mixup + TTA ===\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def _label_smoothing_ce(logits, targets, smoothing=0.1):\n",
    "    if smoothing <= 0:\n",
    "        return F.cross_entropy(logits, targets)\n",
    "    n_class = logits.size(1)\n",
    "    log_prob = F.log_softmax(logits, dim=1)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(log_prob)\n",
    "        true_dist.fill_(smoothing / (n_class - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1 - smoothing)\n",
    "    return torch.mean(torch.sum(-true_dist * log_prob, dim=1))\n",
    "\n",
    "AMP_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def _label_smoothing_ce(logits, targets, smoothing=0.1, weight=None):\n",
    "    n = logits.size(1)\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logp)\n",
    "        true_dist.fill_(smoothing / (n - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1 - smoothing)\n",
    "    loss = torch.sum(-true_dist * logp, dim=1)       # [B]\n",
    "    if weight is not None:\n",
    "        loss = loss * weight[targets]                # 类加权\n",
    "    return loss.mean()\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, lr=2e-4, weight_decay=5e-4, mixup_alpha=0.2, tta_times=4):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        #self.scaler = GradScaler(DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.scaler = GradScaler(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available())\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.tta_times = tta_times\n",
    "\n",
    "    def _mixup(self, x, y, alpha):\n",
    "        if alpha <= 0:\n",
    "            return x, y, 1.0\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        idx = torch.randperm(x.size(0), device=x.device)\n",
    "        x_mix = lam * x + (1 - lam) * x[idx]\n",
    "        return x_mix, (y, y[idx]), lam\n",
    "\n",
    "    def _tta_predict(self, imgs, labels):\n",
    "        # 简单 TTA：水平翻转 + 旋转 90/270\n",
    "        logits_sum = 0\n",
    "        imgs_tta = [imgs,\n",
    "                    torch.flip(imgs, dims=[-1]),\n",
    "                    imgs.transpose(-1, -2),\n",
    "                    torch.flip(imgs.transpose(-1, -2), dims=[-1])]\n",
    "        with torch.no_grad():\n",
    "            for im in imgs_tta[:self.tta_times]:\n",
    "                out = self.model(im, labels=labels, mode='eval')\n",
    "                if isinstance(out, tuple) and len(out)==2:\n",
    "                    _, logits = out\n",
    "                else:\n",
    "                    # Baseline/Innov1 eval 都返回 (res_stack/logits)\n",
    "                    _, logits = out\n",
    "                logits_sum = logits_sum + logits\n",
    "        return logits_sum / len(imgs_tta[:self.tta_times])\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            self.optim.zero_grad(set_to_none=True)\n",
    "            # Mixup\n",
    "            imgs_mix, ys_tuple, lam = self._mixup(imgs, ys, self.mixup_alpha)\n",
    "            #with autocast(DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "            with autocast(AMP_DEVICE, enabled=USE_FP16 and torch.cuda.is_available()):\n",
    "                out = self.model(imgs_mix, labels=ys, mode='train')\n",
    "                if isinstance(out, tuple):\n",
    "                    pred_noise, noise, t, _ = out\n",
    "                    diffusion_loss = self.mse(pred_noise, noise)\n",
    "                else:\n",
    "                    diffusion_loss = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.model, 'reconstruct_residual_fast'):\n",
    "                        res = self.model.reconstruct_residual_fast(imgs_mix, ys, t_start=200, steps=6)\n",
    "                    else:\n",
    "                        res = MultiScaleResidualExtractor(self.model.unet, TIMESTEPS_SAFE).extract_one_timestep(imgs_mix, ys, t_value=200, steps=6)\n",
    "                if hasattr(self.model, 'fusion_classifier'):\n",
    "                    logits = self.model.fusion_classifier(res.unsqueeze(1).to(next(self.model.fusion_classifier.parameters()).device))\n",
    "                else:\n",
    "                    logits = self.model.classifier(res)\n",
    "\n",
    "                if isinstance(ys_tuple, tuple):\n",
    "                    y1, y2 = ys_tuple\n",
    "                    ce = lam * _label_smoothing_ce(logits, y1, smoothing=0.1) + (1-lam) * _label_smoothing_ce(logits, y2, smoothing=0.1)\n",
    "\n",
    "                    # 若有 mixup:\n",
    "                    ce = lam * _label_smoothing_ce(logits, y1, 0.1, CLASS_WEIGHTS_TENSOR)+ (1-lam) * _label_smoothing_ce(logits, y2, 0.1, CLASS_WEIGHTS_TENSOR)\n",
    "                else:\n",
    "                    #ce = _label_smoothing_ce(logits, ys, smoothing=0.1)\n",
    "                    ce = _label_smoothing_ce(logits, ys, smoothing=0.1, weight=CLASS_WEIGHTS_TENSOR)\n",
    "                loss = diffusion_loss + 0.5 * ce\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                total += ys.size(0)\n",
    "                loss_sum += loss.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for imgs, ys in loader:\n",
    "            imgs = imgs.to(DEVICE); ys = ys.to(DEVICE)\n",
    "            logits = self._tta_predict(imgs, ys)\n",
    "            #ce = F.cross_entropy(logits, ys)\n",
    "            ce = F.cross_entropy(logits, ys, weight=CLASS_WEIGHTS_TENSOR)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==ys).sum().item()\n",
    "            total += ys.size(0)\n",
    "            loss_sum += ce.item() * ys.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "print('[Info] Trainer 已增强：LabelSmoothing + Mixup + TTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffbeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "949e065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineFaultDetector params(M): 25.453033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = 'baseline'  # 可选：'baseline' | 'innov1' | 'innov2'\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    model = BaselineFaultDetector(num_classes=len(CLASS_NAMES))\n",
    "elif model_type == 'innov1':\n",
    "    model = Innovation1FaultDetector(num_classes=len(CLASS_NAMES))\n",
    "else:\n",
    "    model = Innovation2FaultDetector(num_classes=len(CLASS_NAMES),\n",
    "                                     use_transformer=False,\n",
    "                                     timesteps=[200],\n",
    "                                     lightweight=True)\n",
    "\n",
    "print(model.__class__.__name__, 'params(M):', sum(p.numel() for p in model.parameters())/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac9d8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 21.7247 acc 0.2487 | val loss 1.9564 acc 0.3113\n",
      "[SAVE] best acc 0.3113 -> ./substation_part2_best.pt\n",
      "Epoch 02 | train loss 0.9761 acc 0.3993 | val loss 1.7539 acc 0.4363\n",
      "[SAVE] best acc 0.4363 -> ./substation_part2_best.pt\n",
      "Epoch 03 | train loss 0.8329 acc 0.4939 | val loss 1.5110 acc 0.4601\n",
      "[SAVE] best acc 0.4601 -> ./substation_part2_best.pt\n",
      "Epoch 04 | train loss 0.7085 acc 0.5681 | val loss 1.4683 acc 0.5192\n",
      "[SAVE] best acc 0.5192 -> ./substation_part2_best.pt\n",
      "Epoch 05 | train loss 0.6853 acc 0.5573 | val loss 1.3227 acc 0.5422\n",
      "[SAVE] best acc 0.5422 -> ./substation_part2_best.pt\n",
      "Epoch 06 | train loss 0.6474 acc 0.5943 | val loss 1.7757 acc 0.5414\n",
      "Epoch 07 | train loss 0.6021 acc 0.6191 | val loss 1.1940 acc 0.6166\n",
      "[SAVE] best acc 0.6166 -> ./substation_part2_best.pt\n",
      "Epoch 08 | train loss 0.5590 acc 0.6398 | val loss 1.2072 acc 0.6143\n",
      "Epoch 09 | train loss 0.5633 acc 0.6436 | val loss 1.2557 acc 0.6089\n",
      "Epoch 10 | train loss 0.5275 acc 0.6630 | val loss 1.1124 acc 0.6304\n",
      "[SAVE] best acc 0.6304 -> ./substation_part2_best.pt\n",
      "Epoch 11 | train loss 0.4873 acc 0.6902 | val loss 1.3017 acc 0.5997\n",
      "Epoch 12 | train loss 0.4935 acc 0.6925 | val loss 1.2623 acc 0.6120\n",
      "Epoch 13 | train loss 0.4703 acc 0.7025 | val loss 1.1445 acc 0.6373\n",
      "[SAVE] best acc 0.6373 -> ./substation_part2_best.pt\n",
      "Epoch 14 | train loss 0.4594 acc 0.7128 | val loss 1.1554 acc 0.6304\n",
      "Epoch 15 | train loss 0.4373 acc 0.7235 | val loss 1.6586 acc 0.6227\n",
      "Epoch 16 | train loss 0.4334 acc 0.7306 | val loss 1.1103 acc 0.6748\n",
      "[SAVE] best acc 0.6748 -> ./substation_part2_best.pt\n",
      "Epoch 17 | train loss 0.4223 acc 0.7287 | val loss 2.4293 acc 0.6196\n",
      "Epoch 18 | train loss 0.4143 acc 0.7373 | val loss 1.2195 acc 0.6311\n",
      "Epoch 19 | train loss 0.4111 acc 0.7460 | val loss 1.1187 acc 0.6518\n",
      "Epoch 20 | train loss 0.3834 acc 0.7602 | val loss 1.1222 acc 0.6664\n",
      "Epoch 21 | train loss 0.3849 acc 0.7627 | val loss 1.1556 acc 0.6695\n",
      "Epoch 22 | train loss 0.3840 acc 0.7642 | val loss 1.1537 acc 0.6748\n",
      "Epoch 23 | train loss 0.3665 acc 0.7768 | val loss 1.1440 acc 0.6656\n",
      "Epoch 24 | train loss 0.3813 acc 0.7617 | val loss 1.4353 acc 0.6089\n",
      "Epoch 25 | train loss 0.3621 acc 0.7722 | val loss 1.1174 acc 0.6787\n",
      "[SAVE] best acc 0.6787 -> ./substation_part2_best.pt\n",
      "Epoch 26 | train loss 0.3304 acc 0.7939 | val loss 1.1006 acc 0.6863\n",
      "[SAVE] best acc 0.6863 -> ./substation_part2_best.pt\n",
      "Epoch 27 | train loss 0.3353 acc 0.7937 | val loss 1.1725 acc 0.6626\n",
      "Epoch 28 | train loss 0.3306 acc 0.7968 | val loss 1.1081 acc 0.6933\n",
      "[SAVE] best acc 0.6933 -> ./substation_part2_best.pt\n",
      "Epoch 29 | train loss 0.3011 acc 0.8186 | val loss 1.2342 acc 0.6817\n",
      "Epoch 30 | train loss 0.3221 acc 0.8048 | val loss 1.2675 acc 0.6610\n",
      "Epoch 31 | train loss 0.3103 acc 0.8127 | val loss 1.0828 acc 0.7002\n",
      "[SAVE] best acc 0.7002 -> ./substation_part2_best.pt\n",
      "Epoch 32 | train loss 0.2867 acc 0.8284 | val loss 1.1245 acc 0.6979\n",
      "Epoch 33 | train loss 0.2874 acc 0.8278 | val loss 1.2074 acc 0.7017\n",
      "[SAVE] best acc 0.7017 -> ./substation_part2_best.pt\n",
      "Epoch 34 | train loss 0.2799 acc 0.8340 | val loss 1.1603 acc 0.7094\n",
      "[SAVE] best acc 0.7094 -> ./substation_part2_best.pt\n",
      "Epoch 35 | train loss 0.2564 acc 0.8493 | val loss 1.1727 acc 0.6994\n",
      "Epoch 36 | train loss 0.2606 acc 0.8445 | val loss 1.2651 acc 0.6963\n",
      "Epoch 37 | train loss 0.2709 acc 0.8405 | val loss 1.1787 acc 0.7032\n",
      "Epoch 38 | train loss 0.2566 acc 0.8478 | val loss 1.0998 acc 0.7163\n",
      "[SAVE] best acc 0.7163 -> ./substation_part2_best.pt\n",
      "Epoch 39 | train loss 0.2305 acc 0.8673 | val loss 1.5319 acc 0.6802\n",
      "Epoch 40 | train loss 0.2662 acc 0.8451 | val loss 1.2017 acc 0.7086\n",
      "Epoch 41 | train loss 0.2634 acc 0.8474 | val loss 1.1356 acc 0.7078\n",
      "Epoch 42 | train loss 0.2569 acc 0.8574 | val loss 1.1701 acc 0.7048\n",
      "Epoch 43 | train loss 0.2386 acc 0.8593 | val loss 1.1625 acc 0.7048\n",
      "Epoch 44 | train loss 0.2296 acc 0.8750 | val loss 1.1639 acc 0.7101\n",
      "Epoch 45 | train loss 0.2202 acc 0.8785 | val loss 1.1241 acc 0.7339\n",
      "[SAVE] best acc 0.7339 -> ./substation_part2_best.pt\n",
      "Epoch 46 | train loss 0.2289 acc 0.8717 | val loss 1.1425 acc 0.7400\n",
      "[SAVE] best acc 0.7400 -> ./substation_part2_best.pt\n",
      "Epoch 47 | train loss 0.2203 acc 0.8771 | val loss 1.9752 acc 0.6917\n",
      "Epoch 48 | train loss 0.2994 acc 0.8252 | val loss 1.3317 acc 0.6817\n",
      "Epoch 49 | train loss 0.2998 acc 0.8202 | val loss 1.2461 acc 0.6810\n",
      "Epoch 50 | train loss 0.2832 acc 0.8372 | val loss 1.4090 acc 0.6863\n"
     ]
    }
   ],
   "source": [
    "# baseline 模型\n",
    "\n",
    "epochs = 50\n",
    "trainer = Trainer(model, lr=2e-4, weight_decay=5e-4)\n",
    "\n",
    "best_acc, best_state = 0.0, None\n",
    "save_path = './substation_part2_best.pt'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = trainer.train_epoch(train_loader)\n",
    "    va_loss, va_acc = trainer.validate(val_loader)\n",
    "    print(f'Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}')\n",
    "    \n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        best_state = {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                      'acc': best_acc, 'classes': list(CLASS_NAMES), 'img_size': img_size}\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f'[SAVE] best acc {best_acc:.4f} -> {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce00368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26225/3365978892.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./substation_part2_best.pt', map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint with acc: 0.6265337423312883\n",
      "Final Val -> loss: 1.7861 acc: 0.6212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('./substation_part2_best.pt'):\n",
    "    ckpt = torch.load('./substation_part2_best.pt', map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    print('Loaded best checkpoint with acc:', ckpt.get('acc', None))\n",
    "else:\n",
    "    print('No checkpoint found, using current model.')\n",
    "\n",
    "va_loss, va_acc = Trainer(model).validate(val_loader)\n",
    "print('Final Val -> loss:', round(va_loss,4), 'acc:', round(va_acc,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6afe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovation1FaultDetector params(M): 25.584446\n"
     ]
    }
   ],
   "source": [
    "# innov1 模型\n",
    "\n",
    "model_type = 'innov1'  # 可选：'baseline' | 'innov1' | 'innov2'\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    model = BaselineFaultDetector(num_classes=len(CLASS_NAMES))\n",
    "elif model_type == 'innov1':\n",
    "    model = Innovation1FaultDetector(num_classes=len(CLASS_NAMES))\n",
    "else:\n",
    "    model = Innovation2FaultDetector(num_classes=len(CLASS_NAMES),\n",
    "                                     use_transformer=False,\n",
    "                                     timesteps=[200],\n",
    "                                     lightweight=True)\n",
    "\n",
    "print(model.__class__.__name__, 'params(M):', sum(p.numel() for p in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15857e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 32.6087 acc 0.1729 | val loss 2.1786 acc 0.1956\n",
      "[SAVE] best acc 0.1956 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 02 | train loss 1.0781 acc 0.2872 | val loss 2.1522 acc 0.2684\n",
      "[SAVE] best acc 0.2684 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 03 | train loss 0.9903 acc 0.3390 | val loss 2.0759 acc 0.3029\n",
      "[SAVE] best acc 0.3029 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 04 | train loss 0.8831 acc 0.4191 | val loss 1.9939 acc 0.3459\n",
      "[SAVE] best acc 0.3459 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 05 | train loss 0.8128 acc 0.4607 | val loss 1.8152 acc 0.4080\n",
      "[SAVE] best acc 0.4080 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 06 | train loss 0.7615 acc 0.4958 | val loss 1.6276 acc 0.4440\n",
      "[SAVE] best acc 0.4440 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 07 | train loss 0.7040 acc 0.5322 | val loss 1.7313 acc 0.4555\n",
      "[SAVE] best acc 0.4555 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 08 | train loss 0.6706 acc 0.5516 | val loss 1.6666 acc 0.4601\n",
      "[SAVE] best acc 0.4601 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 09 | train loss 0.6444 acc 0.5709 | val loss 1.4529 acc 0.5146\n",
      "[SAVE] best acc 0.5146 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 10 | train loss 0.6249 acc 0.5824 | val loss 1.4558 acc 0.5123\n",
      "Epoch 11 | train loss 0.6032 acc 0.6026 | val loss 1.7474 acc 0.4494\n",
      "Epoch 12 | train loss 0.5883 acc 0.6108 | val loss 1.6127 acc 0.4862\n",
      "Epoch 13 | train loss 0.5576 acc 0.6331 | val loss 1.2590 acc 0.5874\n",
      "[SAVE] best acc 0.5874 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 14 | train loss 0.5279 acc 0.6446 | val loss 1.3982 acc 0.5621\n",
      "Epoch 15 | train loss 0.5236 acc 0.6530 | val loss 1.5616 acc 0.4831\n",
      "Epoch 16 | train loss 0.5051 acc 0.6674 | val loss 1.7332 acc 0.5008\n",
      "Epoch 17 | train loss 0.4987 acc 0.6764 | val loss 1.4407 acc 0.5322\n",
      "Epoch 18 | train loss 0.4759 acc 0.6816 | val loss 2.0189 acc 0.4609\n",
      "Epoch 19 | train loss 0.4783 acc 0.6825 | val loss 1.2806 acc 0.5913\n",
      "[SAVE] best acc 0.5913 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 20 | train loss 0.4394 acc 0.7051 | val loss 1.5886 acc 0.5452\n",
      "Epoch 21 | train loss 0.4590 acc 0.6944 | val loss 1.4036 acc 0.5775\n",
      "Epoch 22 | train loss 0.4313 acc 0.7176 | val loss 1.7214 acc 0.4877\n",
      "Epoch 23 | train loss 0.4201 acc 0.7245 | val loss 1.4294 acc 0.5613\n",
      "Epoch 24 | train loss 0.4205 acc 0.7230 | val loss 1.4487 acc 0.5475\n",
      "Epoch 25 | train loss 0.4131 acc 0.7304 | val loss 1.3666 acc 0.5982\n",
      "[SAVE] best acc 0.5982 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 26 | train loss 0.3829 acc 0.7437 | val loss 1.4332 acc 0.5974\n",
      "Epoch 27 | train loss 0.3855 acc 0.7515 | val loss 1.3812 acc 0.5913\n",
      "Epoch 28 | train loss 0.3905 acc 0.7389 | val loss 1.4259 acc 0.5920\n",
      "Epoch 29 | train loss 0.3517 acc 0.7745 | val loss 1.4512 acc 0.5936\n",
      "Epoch 30 | train loss 0.3573 acc 0.7669 | val loss 1.3977 acc 0.6074\n",
      "[SAVE] best acc 0.6074 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 31 | train loss 0.3386 acc 0.7826 | val loss 1.6177 acc 0.5690\n",
      "Epoch 32 | train loss 0.3179 acc 0.7916 | val loss 1.5678 acc 0.6081\n",
      "[SAVE] best acc 0.6081 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 33 | train loss 0.3127 acc 0.7912 | val loss 1.5655 acc 0.5989\n",
      "Epoch 34 | train loss 0.3203 acc 0.7964 | val loss 1.4745 acc 0.6334\n",
      "[SAVE] best acc 0.6334 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 35 | train loss 0.2925 acc 0.8127 | val loss 1.4286 acc 0.6342\n",
      "[SAVE] best acc 0.6342 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 36 | train loss 0.2923 acc 0.8085 | val loss 1.4498 acc 0.6219\n",
      "Epoch 37 | train loss 0.2832 acc 0.8121 | val loss 1.4801 acc 0.6074\n",
      "Epoch 38 | train loss 0.2549 acc 0.8368 | val loss 1.4820 acc 0.6319\n",
      "Epoch 39 | train loss 0.2509 acc 0.8382 | val loss 1.4214 acc 0.6511\n",
      "[SAVE] best acc 0.6511 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 40 | train loss 0.2419 acc 0.8455 | val loss 1.5275 acc 0.6334\n",
      "Epoch 41 | train loss 0.2225 acc 0.8645 | val loss 1.4939 acc 0.6557\n",
      "[SAVE] best acc 0.6557 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 42 | train loss 0.2149 acc 0.8618 | val loss 1.6174 acc 0.6112\n",
      "Epoch 43 | train loss 0.2144 acc 0.8671 | val loss 1.6275 acc 0.6311\n",
      "Epoch 44 | train loss 0.2073 acc 0.8715 | val loss 1.8416 acc 0.6158\n",
      "Epoch 45 | train loss 0.2052 acc 0.8723 | val loss 1.8279 acc 0.5836\n",
      "Epoch 46 | train loss 0.2022 acc 0.8694 | val loss 1.6828 acc 0.6334\n",
      "Epoch 47 | train loss 0.1931 acc 0.8752 | val loss 1.5768 acc 0.6587\n",
      "[SAVE] best acc 0.6587 -> ./innov1_substation_part2_best.pt\n",
      "Epoch 48 | train loss 0.1830 acc 0.8857 | val loss 1.8852 acc 0.6081\n",
      "Epoch 49 | train loss 0.1987 acc 0.8783 | val loss 1.7190 acc 0.6534\n",
      "Epoch 50 | train loss 0.1922 acc 0.8794 | val loss 1.6968 acc 0.6411\n"
     ]
    }
   ],
   "source": [
    "# innov1 模型\n",
    "\n",
    "epochs = 50\n",
    "trainer = Trainer(model, lr=2e-4, weight_decay=5e-4)\n",
    "\n",
    "best_acc, best_state = 0.0, None\n",
    "save_path = './innov1_substation_part2_best.pt'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = trainer.train_epoch(train_loader)\n",
    "    va_loss, va_acc = trainer.validate(val_loader)\n",
    "    print(f'Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}')\n",
    "    \n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        best_state = {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                      'acc': best_acc, 'classes': list(CLASS_NAMES), 'img_size': img_size}\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f'[SAVE] best acc {best_acc:.4f} -> {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b23af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26225/1414661540.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('./innov1_substation_part2_best.pt', map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint with acc: 0.6587423312883436\n",
      "Final Val -> loss: 1.5816 acc: 0.6426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('./innov1_substation_part2_best.pt'):\n",
    "    ckpt = torch.load('./innov1_substation_part2_best.pt', map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    print('Loaded best checkpoint with acc:', ckpt.get('acc', None))\n",
    "else:\n",
    "    print('No checkpoint found, using current model.')\n",
    "\n",
    "va_loss, va_acc = Trainer(model).validate(val_loader)\n",
    "print('Final Val -> loss:', round(va_loss,4), 'acc:', round(va_acc,4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant-py310-test)",
   "language": "python",
   "name": "plant-py310-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
